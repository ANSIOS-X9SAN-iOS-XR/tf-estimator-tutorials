{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimising a TensorFlow SavedModel for Serving"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebooks shows how to optimise the TensorFlow exported SavedModel by **shrinking** its size (to have less memory and disk footprints), and **improving** prediction latency. This can be accopmlished by applying the following:\n",
    "* **Freezing**: That is, converting the variables stored in a checkpoint file of the SavedModel into constants stored directly in the model graph.\n",
    "* **Pruning**: That is, stripping unused nodes during the prediction path of the graph, merging duplicate nodes, as well as removing other node ops like summary, identity, etc.\n",
    "* **Quantisation**:  That is, converting any large float Const op into an eight-bit equivalent, followed by a float conversion op so that the result is usable by subsequent nodes.\n",
    "* **Other refinements**: That includes constant folding, batch_norm folding, fusing convolusion, etc.\n",
    "\n",
    "The optimisation operations we apply in this example are from the TensorFlow [Graph Conversion Tool](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/graph_transforms/README.md#fold_constants), which is a c++ command-line tool. We use the Python APIs to call the c++ libraries. \n",
    "\n",
    "The Graph Transform Tool is designed to work on models that are saved as GraphDef files, usually in a binary protobuf format. However, the model exported after training and estimator is in SavedModel format (saved_model.pb file + variables folder with variables.data-* and variables.index files). \n",
    "\n",
    "We need to optimise the mode and keep it the SavedModel format. Thus, the optimisation steps will be:\n",
    "1. Freeze the SavedModel: SavedModel -> GraphDef\n",
    "2. Optimisae the freezed model: GraphDef -> GraphDef\n",
    "3. Convert the optimised freezed model to SavedModel: GraphDef -> SavedModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow : 1.10.0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import data\n",
    "\n",
    "print \"TensorFlow : {}\".format(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Train and Export a TensorFlow DNNClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST-data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST-data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST-data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST-data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "mnist = tf.contrib.learn.datasets.load_dataset(\"mnist\")\n",
    "train_data = mnist.train.images\n",
    "train_labels = np.asarray(mnist.train.labels, dtype=np.int32)\n",
    "eval_data = mnist.test.images\n",
    "eval_labels = np.asarray(mnist.test.labels, dtype=np.int32)\n",
    "NUM_CLASSES = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data shape: (55000, 784)\n",
      "Eval data shape: (10000, 784)\n"
     ]
    }
   ],
   "source": [
    "print \"Train data shape: {}\".format(train_data.shape)\n",
    "print \"Eval data shape: {}\".format(eval_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Estimator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.1 Model Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_fn(features, labels, mode, params):\n",
    "\n",
    "    # conv layers\n",
    "    def _cnn_layers(input_layer, num_conv_layers, init_filters, mode):\n",
    "\n",
    "        inputs = input_layer\n",
    "\n",
    "        for i in range(num_conv_layers):\n",
    "\n",
    "            current_filters = init_filters * (2**i)\n",
    "            \n",
    "            conv = tf.layers.conv2d(inputs=inputs, kernel_size=3, filters=current_filters, strides=1,\n",
    "                                     padding='SAME', name='conv-{}'.format(i+1))\n",
    "            \n",
    "            pool = tf.layers.max_pooling2d(inputs=conv, pool_size=2, strides=2,\n",
    "                                            padding='SAME', name='pool-{}'.format(i+1))\n",
    "            \n",
    "            batch_norm = tf.layers.batch_normalization(pool, name='batch_norm-{}'.format(i+1))\n",
    "            \n",
    "            if params.debug == True:\n",
    "                tf.summary.histogram('Batch_Normalisation', batch_norm)\n",
    "\n",
    "            if mode==tf.estimator.ModeKeys.TRAIN:\n",
    "                batch_norm = tf.nn.dropout(batch_norm, params.drop_out, name='drop_out-{}'.format(i+1))\n",
    "                \n",
    "            inputs = batch_norm\n",
    "\n",
    "        outputs = batch_norm\n",
    "        return outputs\n",
    "\n",
    "    # model body\n",
    "    def _inference(features, mode, params):\n",
    "        \n",
    "        input_layer = tf.reshape(features[\"input_image\"], [-1, 28, 28, 1], name='input_image')\n",
    "\n",
    "        conv_outputs = _cnn_layers(input_layer, params.num_conv_layers, params.init_filters, mode)\n",
    "        \n",
    "        flatten = tf.layers.flatten(inputs=conv_outputs, name='flatten')\n",
    "        \n",
    "        fully_connected = tf.contrib.layers.stack(inputs=flatten, layer=tf.contrib.layers.fully_connected,\n",
    "                                                stack_args=params.hidden_units,\n",
    "                                                activation_fn=tf.nn.relu)\n",
    "        if params.debug == True:\n",
    "            tf.summary.histogram('Fully_Connected', fully_connected)\n",
    "        \n",
    "        # unused_layer\n",
    "        unused_layers = tf.layers.dense(flatten, units=100, name='unused', activation=tf.nn.relu)\n",
    "        \n",
    "        logits = tf.layers.dense(fully_connected, units=NUM_CLASSES, name='logits', activation=None)\n",
    "        return logits\n",
    "    \n",
    "\n",
    "    # model head\n",
    "    head = tf.contrib.estimator.multi_class_head(n_classes=NUM_CLASSES)\n",
    "    \n",
    "    return head.create_estimator_spec(\n",
    "            features=features,\n",
    "            mode=mode,\n",
    "            logits=_inference(features, mode, params),\n",
    "            labels=labels,\n",
    "            optimizer=tf.train.AdamOptimizer(params.learning_rate)\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.2 Create Custom Estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_estimator(params, run_config):\n",
    "\n",
    "    # evaluation metric_fn\n",
    "    def _metric_fn(labels, predictions):\n",
    "\n",
    "        metrics = {}\n",
    "        pred_class = predictions['class_ids']\n",
    "        metrics['micro_accuracy'] = tf.metrics.mean_per_class_accuracy(\n",
    "            labels=labels, predictions=pred_class, num_classes=NUM_CLASSES\n",
    "        )\n",
    "\n",
    "        return metrics\n",
    "\n",
    "    mnist_classifier = tf.estimator.Estimator(\n",
    "        model_fn=model_fn, params=params, config=run_config)\n",
    "\n",
    "    mnist_classifier = tf.contrib.estimator.add_metrics(\n",
    "        estimator=mnist_classifier, metric_fn=_metric_fn)\n",
    "    \n",
    "    return mnist_classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Train and Evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3.1 Experiment Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment(hparam, run_config):\n",
    "    \n",
    "    train_spec = tf.estimator.TrainSpec(\n",
    "        input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "            x={\"input_image\": train_data},\n",
    "            y=train_labels,\n",
    "            batch_size=hparam.batch_size,\n",
    "            num_epochs=None,\n",
    "            shuffle=True),\n",
    "        max_steps=hparams.max_traning_steps\n",
    "    )\n",
    "\n",
    "    eval_spec = tf.estimator.EvalSpec(\n",
    "        input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "            x={\"input_image\": eval_data},\n",
    "            y=eval_labels,\n",
    "            batch_size=hparam.batch_size,\n",
    "            num_epochs=1,\n",
    "            shuffle=False),\n",
    "        steps=None,\n",
    "        throttle_secs=hparams.eval_throttle_secs\n",
    "    )\n",
    "\n",
    "    tf.logging.set_verbosity(tf.logging.INFO)\n",
    "\n",
    "    time_start = datetime.utcnow() \n",
    "    print(\"Experiment started at {}\".format(time_start.strftime(\"%H:%M:%S\")))\n",
    "    print(\".......................................\") \n",
    "\n",
    "    estimator = create_estimator(hparams, run_config)\n",
    "\n",
    "    tf.estimator.train_and_evaluate(\n",
    "        estimator=estimator,\n",
    "        train_spec=train_spec, \n",
    "        eval_spec=eval_spec\n",
    "    )\n",
    "\n",
    "    time_end = datetime.utcnow() \n",
    "    print(\".......................................\")\n",
    "    print(\"Experiment finished at {}\".format(time_end.strftime(\"%H:%M:%S\")))\n",
    "    print(\"\")\n",
    "    time_elapsed = time_end - time_start\n",
    "    print(\"Experiment elapsed time: {} seconds\".format(time_elapsed.total_seconds()))\n",
    "    \n",
    "    return estimator\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3.2  Experiment Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "models/mnist/cnn_classifier\n"
     ]
    }
   ],
   "source": [
    "MODELS_LOCATION = 'models/mnist'\n",
    "MODEL_NAME = 'cnn_classifier'\n",
    "model_dir = os.path.join(MODELS_LOCATION, MODEL_NAME)\n",
    "\n",
    "print model_dir\n",
    "\n",
    "hparams  = tf.contrib.training.HParams(\n",
    "    batch_size=100,\n",
    "    hidden_units=[1024],\n",
    "    num_conv_layers=2, \n",
    "    init_filters=64,\n",
    "    drop_out=0.85,\n",
    "    max_traning_steps=50,\n",
    "    eval_throttle_secs=10,\n",
    "    learning_rate=1e-3,\n",
    "    debug=True\n",
    ")\n",
    "\n",
    "run_config = tf.estimator.RunConfig(\n",
    "    tf_random_seed=19830610,\n",
    "    save_checkpoints_steps=1000,\n",
    "    keep_checkpoint_max=3,\n",
    "    model_dir=model_dir\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TensorFlow Graph \n",
    "![](tf-graph.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3.3 Run Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing previous artifacts...\n",
      "Experiment started at 17:22:42\n",
      ".......................................\n",
      "INFO:tensorflow:Using config: {'_save_checkpoints_secs': None, '_global_id_in_cluster': 0, '_session_config': None, '_keep_checkpoint_max': 3, '_tf_random_seed': 19830610, '_task_type': 'worker', '_train_distribute': None, '_is_chief': True, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x1231c90d0>, '_model_dir': 'models/mnist/cnn_classifier', '_num_worker_replicas': 1, '_task_id': 0, '_log_step_count_steps': 100, '_master': '', '_save_checkpoints_steps': 1000, '_keep_checkpoint_every_n_hours': 10000, '_evaluation_master': '', '_service': None, '_device_fn': None, '_save_summary_steps': 100, '_num_ps_replicas': 0}\n",
      "INFO:tensorflow:Using config: {'_save_checkpoints_secs': None, '_session_config': None, '_keep_checkpoint_max': 3, '_task_type': 'worker', '_global_id_in_cluster': 0, '_is_chief': True, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x116a9af90>, '_evaluation_master': '', '_save_checkpoints_steps': 1000, '_keep_checkpoint_every_n_hours': 10000, '_service': None, '_num_ps_replicas': 0, '_tf_random_seed': 19830610, '_master': '', '_device_fn': None, '_num_worker_replicas': 1, '_task_id': 0, '_log_step_count_steps': 100, '_model_dir': 'models/mnist/cnn_classifier', '_train_distribute': None, '_save_summary_steps': 100}\n",
      "INFO:tensorflow:Running training and evaluation locally (non-distributed).\n",
      "INFO:tensorflow:Start train and evaluate loop. The evaluate will happen after every checkpoint. Checkpoint frequency is determined based on RunConfig arguments: save_checkpoints_steps 1000 or save_checkpoints_secs None.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 0 into models/mnist/cnn_classifier/model.ckpt.\n",
      "INFO:tensorflow:loss = 2.2921264, step = 1\n",
      "INFO:tensorflow:Saving checkpoints for 50 into models/mnist/cnn_classifier/model.ckpt.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2018-09-13-17:23:05\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from models/mnist/cnn_classifier/model.ckpt-50\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Finished evaluation at 2018-09-13-17:24:04\n",
      "INFO:tensorflow:Saving dict for global step 50: accuracy = 0.92016363, average_loss = 0.25640455, global_step = 50, loss = 0.25640467, micro_accuracy = 0.9190322\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 50: models/mnist/cnn_classifier/model.ckpt-50\n",
      "INFO:tensorflow:Loss for final step: 0.19030952.\n",
      ".......................................\n",
      "Experiment finished at 17:24:04\n",
      "\n",
      "Experiment elapsed time: 81.96476 seconds\n"
     ]
    }
   ],
   "source": [
    "if tf.gfile.Exists(model_dir):\n",
    "    print(\"Removing previous artifacts...\")\n",
    "    tf.gfile.DeleteRecursively(model_dir)\n",
    "    \n",
    "estimator = run_experiment(hparams, run_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Export the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Eval: None\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Classify: None\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Regress: None\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Predict: ['predict']\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Train: None\n",
      "INFO:tensorflow:Signatures EXCLUDED from export because they cannot be be served via TensorFlow Serving APIs:\n",
      "INFO:tensorflow:'serving_default' : Classification input must be a single string Tensor; got {'input_image': <tf.Tensor 'input_image:0' shape=(?, 784) dtype=float32>}\n",
      "INFO:tensorflow:'classification' : Classification input must be a single string Tensor; got {'input_image': <tf.Tensor 'input_image:0' shape=(?, 784) dtype=float32>}\n",
      "WARNING:tensorflow:Export includes no default signature!\n",
      "INFO:tensorflow:Restoring parameters from models/mnist/cnn_classifier/model.ckpt-50\n",
      "INFO:tensorflow:Assets added to graph.\n",
      "INFO:tensorflow:No assets to write.\n",
      "INFO:tensorflow:SavedModel written to: models/mnist/cnn_classifier/export/temp-1536860075/saved_model.pb\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'models/mnist/cnn_classifier/export/1536860075'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def make_serving_input_receiver_fn():\n",
    "    inputs = {'input_image': tf.placeholder(shape=[None,784], dtype=tf.float32, name='input_image')}\n",
    "    return tf.estimator.export.build_raw_serving_input_receiver_fn(inputs)\n",
    "\n",
    "export_dir = os.path.join(model_dir, 'export')\n",
    "\n",
    "if tf.gfile.Exists(export_dir):\n",
    "    tf.gfile.DeleteRecursively(export_dir)\n",
    "        \n",
    "estimator.export_savedmodel(\n",
    "    export_dir_base=export_dir,\n",
    "    serving_input_receiver_fn=make_serving_input_receiver_fn()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Inspect the Exported SavedModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "models/mnist/cnn_classifier/export/1536860075\n",
      "saved_model.pb\n",
      "variables\n",
      "\n",
      "MetaGraphDef with tag-set: 'serve' contains the following SignatureDefs:\n",
      "\n",
      "signature_def['predict']:\n",
      "  The given SavedModel SignatureDef contains the following input(s):\n",
      "    inputs['input_image'] tensor_info:\n",
      "        dtype: DT_FLOAT\n",
      "        shape: (-1, 784)\n",
      "        name: input_image:0\n",
      "  The given SavedModel SignatureDef contains the following output(s):\n",
      "    outputs['class_ids'] tensor_info:\n",
      "        dtype: DT_INT64\n",
      "        shape: (-1, 1)\n",
      "        name: head/predictions/ExpandDims:0\n",
      "    outputs['classes'] tensor_info:\n",
      "        dtype: DT_STRING\n",
      "        shape: (-1, 1)\n",
      "        name: head/predictions/str_classes:0\n",
      "    outputs['logits'] tensor_info:\n",
      "        dtype: DT_FLOAT\n",
      "        shape: (-1, 10)\n",
      "        name: logits/BiasAdd:0\n",
      "    outputs['probabilities'] tensor_info:\n",
      "        dtype: DT_FLOAT\n",
      "        shape: (-1, 10)\n",
      "        name: head/predictions/probabilities:0\n",
      "  Method name is: tensorflow/serving/predict\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "saved_models_base=models/mnist/cnn_classifier/export/\n",
    "saved_model_dir=${saved_models_base}$(ls ${saved_models_base} | tail -n 1)\n",
    "echo ${saved_model_dir}\n",
    "ls ${saved_model_dir}\n",
    "saved_model_cli show --dir=${saved_model_dir} --all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction with SavedModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference_test(saved_model_dir, signature=\"predict\", batch=200, repeat=200):\n",
    "\n",
    "    tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "    \n",
    "    \n",
    "    time_start = datetime.utcnow() \n",
    "    \n",
    "    predictor = tf.contrib.predictor.from_saved_model(\n",
    "        export_dir = saved_model_dir,\n",
    "        signature_def_key=signature\n",
    "    )\n",
    "    time_end = datetime.utcnow() \n",
    "        \n",
    "    time_elapsed = time_end - time_start\n",
    "   \n",
    "    print \"\"\n",
    "    print(\"Model loading time: {} seconds\".format(time_elapsed.total_seconds()))\n",
    "    print \"\"\n",
    "    \n",
    "    time_start = datetime.utcnow() \n",
    "    output = None\n",
    "    for i in range(repeat):\n",
    "        output = predictor(\n",
    "            {\n",
    "                'input_image': eval_data[:batch]\n",
    "            }\n",
    "        )\n",
    "    \n",
    "    time_end = datetime.utcnow() \n",
    "\n",
    "    time_elapsed_sec = (time_end - time_start).total_seconds()\n",
    "    \n",
    "    print \"Inference elapsed time: {} seconds\".format(time_elapsed_sec)\n",
    "    print \"\"\n",
    "    \n",
    "    print \"Prediction produced for {} instances batch, repeated {} times\".format(len(output['class_ids']), repeat)\n",
    "    print \"Average latency per batch: {} seconds\".format(time_elapsed_sec/repeat)\n",
    "    print \"\"\n",
    "    \n",
    "    print \"Prediction output for the last instance:\"\n",
    "    for key in output.keys():\n",
    "        print \"{}: {}\".format(key,output[key][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Test Prediction with SavedModel "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "models/mnist/cnn_classifier/export/1536860075\n",
      "\n",
      "Model loading time: 0.309946 seconds\n",
      "\n",
      "Inference elapsed time: 39.040816 seconds\n",
      "\n",
      "Prediction produced for 200 instances batch, repeated 200 times\n",
      "Average latency per batch: 0.19520408 seconds\n",
      "\n",
      "Prediction output for the last instance:\n",
      "probabilities: [7.6724064e-06 2.6197768e-08 1.1787332e-06 5.1102885e-05 1.3447743e-07\n",
      " 2.8202882e-07 9.4968700e-10 9.9980205e-01 9.3225651e-07 1.3655354e-04]\n",
      "class_ids: [7]\n",
      "classes: ['7']\n",
      "logits: [ 0.9675203 -4.7121897 -0.9056696  2.863731  -3.0764687 -2.3358555\n",
      " -8.029488  12.745203  -1.1402568  3.8466072]\n"
     ]
    }
   ],
   "source": [
    "saved_model_dir = os.path.join(export_dir, os.listdir(export_dir)[-1]) \n",
    "print(saved_model_dir)\n",
    "inference_test(saved_model_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Describe GraphDef"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def describe_graph(graph_def, show_nodes=False):\n",
    "    \n",
    "    print 'Input Feature Nodes: {}'.format([node.name for node in graph_def.node if node.op=='Placeholder'])\n",
    "    print \"\"\n",
    "    print 'Unused Nodes: {}'.format([node.name for node in graph_def.node if 'unused'  in node.name])\n",
    "    print \"\"\n",
    "    print 'Output Nodes: {}'.format( [node.name for node in graph_def.node if 'predictions' in node.name])\n",
    "    print \"\"\n",
    "    print 'Quanitization Nodes: {}'.format( [node.name for node in graph_def.node if 'quant' in node.name])\n",
    "    print \"\"\n",
    "    print 'Constant Count: {}'.format( len([node for node in graph_def.node if node.op=='Const']))\n",
    "    print \"\"\n",
    "    print 'Variable Count: {}'.format( len([node for node in graph_def.node if 'Variable' in node.op]))\n",
    "    print \"\"\n",
    "    print 'Identity Count: {}'.format( len([node for node in graph_def.node if node.op=='Identity']))\n",
    "    print \"\"\n",
    "    print 'Total nodes: {}'.format( len(graph_def.node))\n",
    "    print ''\n",
    "    \n",
    "    if show_nodes==True:\n",
    "        for node in graph_def.node:\n",
    "            print 'Op:{} - Name: {}'.format(node.op, node.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Describe the SavedModel Graph (before optimisation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load GraphDef from a SavedModel Directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_graph_def_from_saved_model(saved_model_dir):\n",
    "    \n",
    "    print saved_model_dir\n",
    "    print \"\"\n",
    "    \n",
    "    from tensorflow.python.saved_model import tag_constants\n",
    "    \n",
    "    with tf.Session() as session:\n",
    "        meta_graph_def = tf.saved_model.loader.load(\n",
    "            session,\n",
    "            tags=[tag_constants.SERVING],\n",
    "            export_dir=saved_model_dir\n",
    "        )\n",
    "        \n",
    "    return meta_graph_def.graph_def"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "models/mnist/cnn_classifier/export/1536860075\n",
      "\n",
      "Input Feature Nodes: [u'input_image']\n",
      "\n",
      "Unused Nodes: [u'unused/kernel/Initializer/random_uniform/shape', u'unused/kernel/Initializer/random_uniform/min', u'unused/kernel/Initializer/random_uniform/max', u'unused/kernel/Initializer/random_uniform/RandomUniform', u'unused/kernel/Initializer/random_uniform/sub', u'unused/kernel/Initializer/random_uniform/mul', u'unused/kernel/Initializer/random_uniform', u'unused/kernel', u'unused/kernel/Assign', u'unused/kernel/read', u'unused/bias/Initializer/zeros', u'unused/bias', u'unused/bias/Assign', u'unused/bias/read', u'unused/MatMul', u'unused/BiasAdd', u'unused/Relu']\n",
      "\n",
      "Output Nodes: [u'head/predictions/class_ids/dimension', u'head/predictions/class_ids', u'head/predictions/ExpandDims/dim', u'head/predictions/ExpandDims', u'head/predictions/str_classes', u'head/predictions/probabilities']\n",
      "\n",
      "Quanitization Nodes: []\n",
      "\n",
      "Constant Count: 61\n",
      "\n",
      "Variable Count: 19\n",
      "\n",
      "Identity Count: 21\n",
      "\n",
      "Total nodes: 211\n",
      "\n"
     ]
    }
   ],
   "source": [
    "describe_graph(get_graph_def_from_saved_model(saved_model_dir))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get model size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_size(model_dir):\n",
    "    \n",
    "    print model_dir\n",
    "    print \"\"\n",
    "    \n",
    "    pb_size = os.path.getsize(os.path.join(model_dir,'saved_model.pb'))\n",
    "    \n",
    "    variables_size = 0\n",
    "    if os.path.exists(os.path.join(model_dir,'variables/variables.data-00000-of-00001')):\n",
    "        variables_size = os.path.getsize(os.path.join(model_dir,'variables/variables.data-00000-of-00001'))\n",
    "        variables_size += os.path.getsize(os.path.join(model_dir,'variables/variables.index'))\n",
    "\n",
    "    print \"Model size: {} KB\".format(round(pb_size/(1024.0),3))\n",
    "    print \"Variables size: {} KB\".format(round( variables_size/(1024.0),3))\n",
    "    print \"Total Size: {} KB\".format(round((pb_size + variables_size)/(1024.0),3))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "models/mnist/cnn_classifier/export/1536860075\n",
      "\n",
      "Model size: 43.462 KB\n",
      "Variables size: 27877.204 KB\n",
      "Total Size: 27920.666 KB\n"
     ]
    }
   ],
   "source": [
    "get_size(saved_model_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Freeze SavedModel\n",
    "\n",
    "This function will convert the SavedModel into a GraphDef file (freezed_model.pb), and storing the variables as constrant to the freezed_model.pb\n",
    "\n",
    "You need to define the graph output nodes for freezing. We are only interested in the **class_id**, which is produced by **head/predictions/ExpandDims** node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def freeze_graph(saved_model_dir):\n",
    "    \n",
    "    from tensorflow.python.tools import freeze_graph\n",
    "    from tensorflow.python.saved_model import tag_constants\n",
    "    \n",
    "    output_graph_filename = os.path.join(saved_model_dir, \"freezed_model.pb\")\n",
    "    output_node_names = \"head/predictions/ExpandDims\"\n",
    "    initializer_nodes = \"\"\n",
    "\n",
    "    freeze_graph.freeze_graph(\n",
    "        input_saved_model_dir=saved_model_dir,\n",
    "        output_graph=output_graph_filename,\n",
    "        saved_model_tags = tag_constants.SERVING,\n",
    "        output_node_names=output_node_names,\n",
    "        initializer_nodes=initializer_nodes,\n",
    "\n",
    "        input_graph=None, \n",
    "        input_saver=False,\n",
    "        input_binary=False, \n",
    "        input_checkpoint=None, \n",
    "        restore_op_name=None, \n",
    "        filename_tensor_name=None, \n",
    "        clear_devices=False,\n",
    "        input_meta_graph=False,\n",
    "    )\n",
    "    \n",
    "    print \"SavedModel graph freezed!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SavedModel graph freezed!\n"
     ]
    }
   ],
   "source": [
    "freeze_graph(saved_model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "models/mnist/cnn_classifier/export/1536860075\n",
      "freezed_model.pb\n",
      "saved_model.pb\n",
      "variables\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "saved_models_base=models/mnist/cnn_classifier/export/\n",
    "saved_model_dir=${saved_models_base}$(ls ${saved_models_base} | tail -n 1)\n",
    "echo ${saved_model_dir}\n",
    "ls ${saved_model_dir}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Describe the freezed_model.pb Graph (after freezing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load GraphDef from GraphDef File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_graph_def_from_file(graph_filepath):\n",
    "    \n",
    "    print graph_filepath\n",
    "    print \"\"\n",
    "    \n",
    "    from tensorflow.python import ops\n",
    "    \n",
    "    with ops.Graph().as_default():\n",
    "        with tf.gfile.GFile(graph_filepath, \"rb\") as f:\n",
    "            graph_def = tf.GraphDef()\n",
    "            graph_def.ParseFromString(f.read())\n",
    "            \n",
    "            return graph_def\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "models/mnist/cnn_classifier/export/1536860075/freezed_model.pb\n",
      "\n",
      "Input Feature Nodes: [u'input_image']\n",
      "\n",
      "Unused Nodes: []\n",
      "\n",
      "Output Nodes: [u'head/predictions/class_ids/dimension', u'head/predictions/class_ids', u'head/predictions/ExpandDims/dim', u'head/predictions/ExpandDims']\n",
      "\n",
      "Quanitization Nodes: []\n",
      "\n",
      "Constant Count: 23\n",
      "\n",
      "Variable Count: 0\n",
      "\n",
      "Identity Count: 16\n",
      "\n",
      "Total nodes: 60\n",
      "\n"
     ]
    }
   ],
   "source": [
    "freezed_filepath=os.path.join(saved_model_dir,'freezed_model.pb')\n",
    "describe_graph(get_graph_def_from_file(freezed_filepath))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Optimise the freezed_model.pb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimise GraphDef"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_graph(model_dir, graph_filename, transforms):\n",
    "    \n",
    "    from tensorflow.tools.graph_transforms import TransformGraph\n",
    "    \n",
    "    input_names = []\n",
    "    output_names = ['head/predictions/ExpandDims']\n",
    "    \n",
    "    graph_def = get_graph_def_from_file(os.path.join(model_dir, graph_filename))\n",
    "    optimised_graph_def = TransformGraph(graph_def, \n",
    "                                         input_names,\n",
    "                                         output_names,\n",
    "                                         transforms \n",
    "                                        )\n",
    "    tf.train.write_graph(optimised_graph_def,\n",
    "                        logdir=model_dir,\n",
    "                        as_text=False,\n",
    "                        name='optimised_model.pb')\n",
    "    \n",
    "    print \"Freezed graph optimised!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "models/mnist/cnn_classifier/export/1536860075/freezed_model.pb\n",
      "\n",
      "Freezed graph optimised!\n"
     ]
    }
   ],
   "source": [
    "transforms = [\n",
    "    'remove_nodes(op=Identity)', \n",
    "    'fold_constants(ignore_errors=true)',\n",
    "    'fold_batch_norms',\n",
    "#     'quantize_weights',\n",
    "#     'quantize_nodes',\n",
    "    'merge_duplicate_nodes',\n",
    "    'strip_unused_nodes', \n",
    "    'sort_by_execution_order'\n",
    "]\n",
    "\n",
    "optimize_graph(saved_model_dir, 'freezed_model.pb', transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "models/mnist/cnn_classifier/export/1536860075\n",
      "freezed_model.pb\n",
      "optimised\n",
      "optimised_model.pb\n",
      "saved_model.pb\n",
      "variables\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "saved_models_base=models/mnist/cnn_classifier/export/\n",
    "saved_model_dir=${saved_models_base}$(ls ${saved_models_base} | tail -n 1)\n",
    "echo ${saved_model_dir}\n",
    "ls ${saved_model_dir}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Describe the Optimised Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "models/mnist/cnn_classifier/export/1536860075/optimised_model.pb\n",
      "\n",
      "Input Feature Nodes: [u'input_image']\n",
      "\n",
      "Unused Nodes: []\n",
      "\n",
      "Output Nodes: [u'head/predictions/class_ids', u'head/predictions/ExpandDims']\n",
      "\n",
      "Quanitization Nodes: []\n",
      "\n",
      "Constant Count: 20\n",
      "\n",
      "Variable Count: 0\n",
      "\n",
      "Identity Count: 0\n",
      "\n",
      "Total nodes: 41\n",
      "\n"
     ]
    }
   ],
   "source": [
    "optimised_filepath=os.path.join(saved_model_dir,'optimised_model.pb')\n",
    "describe_graph(get_graph_def_from_file(optimised_filepath))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Convert Optimised graph (GraphDef) to SavedModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_graph_def_to_saved_model(graph_filepath):\n",
    "\n",
    "    from tensorflow.python import ops\n",
    "    export_dir=os.path.join(saved_model_dir,'optimised')\n",
    "\n",
    "    if tf.gfile.Exists(export_dir):\n",
    "        tf.gfile.DeleteRecursively(export_dir)\n",
    "\n",
    "    graph_def = get_graph_def_from_file(graph_filepath)\n",
    "    \n",
    "    with tf.Session(graph=tf.Graph()) as session:\n",
    "        tf.import_graph_def(graph_def, name=\"\")\n",
    "        tf.saved_model.simple_save(session,\n",
    "                export_dir,\n",
    "                inputs={\n",
    "                    node.name: session.graph.get_tensor_by_name(\"{}:0\".format(node.name)) \n",
    "                    for node in graph_def.node if node.op=='Placeholder'},\n",
    "                outputs={\n",
    "                    \"class_ids\": session.graph.get_tensor_by_name(\"head/predictions/ExpandDims:0\"),\n",
    "                }\n",
    "            )\n",
    "\n",
    "        print \"Optimised graph converted to SavedModel!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "models/mnist/cnn_classifier/export/1536860075/optimised_model.pb\n",
      "\n",
      "Optimised graph converted to SavedModel!\n"
     ]
    }
   ],
   "source": [
    "optimised_filepath=os.path.join(saved_model_dir,'optimised_model.pb')\n",
    "convert_graph_def_to_saved_model(optimised_filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimised SavedModel Size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "models/mnist/cnn_classifier/export/1536860075/optimised\n",
      "\n",
      "Model size: 25433.836 KB\n",
      "Variables size: 0.0 KB\n",
      "Total Size: 25433.836 KB\n"
     ]
    }
   ],
   "source": [
    "optimised_saved_model_dir = os.path.join(saved_model_dir,'optimised') \n",
    "get_size(optimised_saved_model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved_model.pb\n",
      "variables\n",
      "\n",
      "MetaGraphDef with tag-set: 'serve' contains the following SignatureDefs:\n",
      "\n",
      "signature_def['serving_default']:\n",
      "  The given SavedModel SignatureDef contains the following input(s):\n",
      "    inputs['input_image'] tensor_info:\n",
      "        dtype: DT_FLOAT\n",
      "        shape: (-1, 784)\n",
      "        name: input_image:0\n",
      "  The given SavedModel SignatureDef contains the following output(s):\n",
      "    outputs['class_ids'] tensor_info:\n",
      "        dtype: DT_INT64\n",
      "        shape: (-1, 1)\n",
      "        name: head/predictions/ExpandDims:0\n",
      "  Method name is: tensorflow/serving/predict\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "saved_models_base=models/mnist/cnn_classifier/export/\n",
    "saved_model_dir=${saved_models_base}$(ls ${saved_models_base} | tail -n 1)/optimised\n",
    "ls ${saved_model_dir}\n",
    "saved_model_cli show --dir ${saved_model_dir} --all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Prediction with the Optimised SavedModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "models/mnist/cnn_classifier/export/1536860075/optimised\n",
      "\n",
      "Model loading time: 0.231705 seconds\n",
      "\n",
      "Inference elapsed time: 34.069475 seconds\n",
      "\n",
      "Prediction produced for 200 instances batch, repeated 200 times\n",
      "Average latency per batch: 0.170347375 seconds\n",
      "\n",
      "Prediction output for the last instance:\n",
      "class_ids: [7]\n"
     ]
    }
   ],
   "source": [
    "freezed_saved_model_dir = os.path.join(saved_model_dir,'optimised') \n",
    "print(freezed_saved_model_dir)\n",
    "inference_test(saved_model_dir=freezed_saved_model_dir, signature='serving_default')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cloud ML Engine Deployment and Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT = 'ksalama-gcp-playground'\n",
    "BUCKET = 'ksalama-gcs-cloudml'\n",
    "REGION = 'europe-west1'\n",
    "MODEL_NAME = 'mnist_classifier'\n",
    "\n",
    "os.environ['BUCKET'] = BUCKET\n",
    "os.environ['PROJECT'] = PROJECT\n",
    "os.environ['REGION'] = REGION\n",
    "os.environ['MODEL_NAME'] = MODEL_NAME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Upload the model artefacts to Google Cloud Storage bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CommandException: 1 files/objects could not be removed.\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "gsutil -m rm -r gs://${BUCKET}/tf-model-optimisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "models/mnist/cnn_classifier/export/1536860075\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Copying file://models/mnist/cnn_classifier/export/1536860075/optimised_model.pb [Content-Type=application/octet-stream]...\n",
      "Resuming upload for file://models/mnist/cnn_classifier/export/1536860075/optimised_model.pb\n",
      "Copying file://models/mnist/cnn_classifier/export/1536860075/optimised/saved_model.pb [Content-Type=application/octet-stream]...\n",
      "Copying file://models/mnist/cnn_classifier/export/1536860075/freezed_model.pb [Content-Type=application/octet-stream]...\n",
      "Resuming upload for file://models/mnist/cnn_classifier/export/1536860075/optimised/saved_model.pb\n",
      "Resuming upload for file://models/mnist/cnn_classifier/export/1536860075/freezed_model.pb\n",
      "/ [0/6 files][    0.0 B/101.8 MiB]   0% Done                                    \r",
      "/ [0/6 files][    0.0 B/101.8 MiB]   0% Done                                    \r",
      "/ [0/6 files][    0.0 B/101.8 MiB]   0% Done                                    \r",
      "Copying file://models/mnist/cnn_classifier/export/1536860075/variables/variables.index [Content-Type=application/octet-stream]...\n",
      "/ [0/6 files][    0.0 B/101.8 MiB]   0% Done                                    \r",
      "Copying file://models/mnist/cnn_classifier/export/1536860075/saved_model.pb [Content-Type=application/octet-stream]...\n",
      "/ [0/6 files][    0.0 B/101.8 MiB]   0% Done                                    \r",
      "Copying file://models/mnist/cnn_classifier/export/1536860075/variables/variables.data-00000-of-00001 [Content-Type=application/octet-stream]...\n",
      "/ [0/6 files][    0.0 B/101.8 MiB]   0% Done                                    \r",
      "Resuming upload for file://models/mnist/cnn_classifier/export/1536860075/variables/variables.data-00000-of-00001\n",
      "/ [1/6 files][ 44.2 KiB/101.8 MiB]   0% Done                                    \r",
      "/ [2/6 files][ 21.8 MiB/101.8 MiB]  21% Done                                    \r",
      "-\r",
      "- [2/6 files][ 31.3 MiB/101.8 MiB]  30% Done                                    \r",
      "\\\r",
      "\\ [2/6 files][ 32.1 MiB/101.8 MiB]  31% Done                                    \r",
      "|\r",
      "| [2/6 files][ 32.3 MiB/101.8 MiB]  31% Done                                    \r",
      "/\r",
      "/ [2/6 files][ 32.6 MiB/101.8 MiB]  32% Done                                    \r",
      "-\r",
      "- [2/6 files][ 32.9 MiB/101.8 MiB]  32% Done                                    \r",
      "\\\r",
      "|\r",
      "| [2/6 files][ 33.6 MiB/101.8 MiB]  33% Done                                    \r",
      "/\r",
      "/ [2/6 files][ 33.9 MiB/101.8 MiB]  33% Done 250.8 KiB/s ETA 00:04:37           \r",
      "-\r",
      "- [2/6 files][ 34.1 MiB/101.8 MiB]  33% Done 173.6 KiB/s ETA 00:06:39           \r",
      "\\\r",
      "\\ [2/6 files][ 34.4 MiB/101.8 MiB]  33% Done 180.5 KiB/s ETA 00:06:22           \r",
      "|\r",
      "/\r",
      "/ [2/6 files][ 34.9 MiB/101.8 MiB]  34% Done 228.4 KiB/s ETA 00:05:00           \r",
      "-\r",
      "- [2/6 files][ 35.2 MiB/101.8 MiB]  34% Done 193.5 KiB/s ETA 00:05:52           \r",
      "\\\r",
      "|\r",
      "| [2/6 files][ 35.9 MiB/101.8 MiB]  35% Done 189.7 KiB/s ETA 00:05:55           \r",
      "/\r",
      "/ [2/6 files][ 36.2 MiB/101.8 MiB]  35% Done 109.6 KiB/s ETA 00:10:13           \r",
      "-\r",
      "- [2/6 files][ 36.7 MiB/101.8 MiB]  36% Done 293.9 KiB/s ETA 00:03:47           \r",
      "\\\r",
      "\\ [2/6 files][ 37.0 MiB/101.8 MiB]  36% Done 222.8 KiB/s ETA 00:04:58           \r",
      "|\r",
      "/\r",
      "/ [2/6 files][ 37.8 MiB/101.8 MiB]  37% Done 203.3 KiB/s ETA 00:05:23           \r",
      "-\r",
      "\\\r",
      "\\ [2/6 files][ 38.5 MiB/101.8 MiB]  37% Done 288.0 KiB/s ETA 00:03:45           \r",
      "|\r",
      "| [2/6 files][ 38.8 MiB/101.8 MiB]  38% Done 255.2 KiB/s ETA 00:04:13           \r",
      "/\r",
      "/ [2/6 files][ 39.0 MiB/101.8 MiB]  38% Done 166.8 KiB/s ETA 00:06:25           \r",
      "-\r",
      "- [2/6 files][ 39.6 MiB/101.8 MiB]  38% Done 237.5 KiB/s ETA 00:04:28           \r",
      "\\\r",
      "\\ [2/6 files][ 39.8 MiB/101.8 MiB]  39% Done 242.0 KiB/s ETA 00:04:22           \r",
      "|\r",
      "| [2/6 files][ 40.1 MiB/101.8 MiB]  39% Done 140.9 KiB/s ETA 00:07:29           \r",
      "/\r",
      "/ [2/6 files][ 40.6 MiB/101.8 MiB]  39% Done 213.3 KiB/s ETA 00:04:54           \r",
      "-\r",
      "- [2/6 files][ 41.1 MiB/101.8 MiB]  40% Done 281.2 KiB/s ETA 00:03:41           \r",
      "\\\r",
      "\\ [2/6 files][ 41.4 MiB/101.8 MiB]  40% Done 187.3 KiB/s ETA 00:05:30           \r",
      "|\r",
      "/\r",
      "/ [2/6 files][ 41.9 MiB/101.8 MiB]  41% Done 215.7 KiB/s ETA 00:04:44           \r",
      "-\r",
      "- [2/6 files][ 42.1 MiB/101.8 MiB]  41% Done 224.4 KiB/s ETA 00:04:32           \r",
      "\\\r",
      "\\ [2/6 files][ 42.6 MiB/101.8 MiB]  41% Done 201.3 KiB/s ETA 00:05:01           \r",
      "|\r",
      "/\r",
      "/ [2/6 files][ 43.2 MiB/101.8 MiB]  42% Done 245.6 KiB/s ETA 00:04:04           \r",
      "-\r",
      "- [2/6 files][ 43.4 MiB/101.8 MiB]  42% Done 249.2 KiB/s ETA 00:04:00           \r",
      "\\\r",
      "\\ [2/6 files][ 43.7 MiB/101.8 MiB]  42% Done 229.9 KiB/s ETA 00:04:19           \r",
      "|\r",
      "| [2/6 files][ 44.2 MiB/101.8 MiB]  43% Done 212.4 KiB/s ETA 00:04:38           \r",
      "/\r",
      "-\r",
      "- [2/6 files][ 44.7 MiB/101.8 MiB]  43% Done 239.0 KiB/s ETA 00:04:05           \r",
      "\\\r",
      "\\ [2/6 files][ 45.0 MiB/101.8 MiB]  44% Done 206.7 KiB/s ETA 00:04:42           \r",
      "|\r",
      "| [2/6 files][ 45.2 MiB/101.8 MiB]  44% Done 188.4 KiB/s ETA 00:05:07           \r",
      "/\r",
      "/ [2/6 files][ 45.7 MiB/101.8 MiB]  44% Done 260.9 KiB/s ETA 00:03:40           \r",
      "-\r",
      "- [2/6 files][ 46.0 MiB/101.8 MiB]  45% Done 232.2 KiB/s ETA 00:04:06           \r",
      "\\\r",
      "\\ [2/6 files][ 46.3 MiB/101.8 MiB]  45% Done 234.7 KiB/s ETA 00:04:02           \r",
      "|\r",
      "| [2/6 files][ 46.8 MiB/101.8 MiB]  45% Done 196.7 KiB/s ETA 00:04:46           \r",
      "/\r",
      "-\r",
      "- [2/6 files][ 47.5 MiB/101.8 MiB]  46% Done 267.6 KiB/s ETA 00:03:28           \r",
      "\\\r",
      "|\r",
      "| [2/6 files][ 48.1 MiB/101.8 MiB]  47% Done 211.5 KiB/s ETA 00:04:20           \r",
      "/\r",
      "-\r",
      "- [2/6 files][ 48.6 MiB/101.8 MiB]  47% Done 233.2 KiB/s ETA 00:03:54           \r",
      "\\\r",
      "|\r",
      "| [2/6 files][ 49.1 MiB/101.8 MiB]  48% Done 180.1 KiB/s ETA 00:05:00           \r",
      "/\r",
      "-\r",
      "- [2/6 files][ 49.9 MiB/101.8 MiB]  48% Done 325.3 KiB/s ETA 00:02:43           \r",
      "\\\r",
      "\\ [2/6 files][ 50.1 MiB/101.8 MiB]  49% Done 162.1 KiB/s ETA 00:05:26           \r",
      "|\r",
      "| [2/6 files][ 50.9 MiB/101.8 MiB]  50% Done 288.7 KiB/s ETA 00:03:00           \r",
      "/\r",
      "-\r",
      "- [2/6 files][ 51.4 MiB/101.8 MiB]  50% Done 334.7 KiB/s ETA 00:02:34           \r",
      "- [2/6 files][ 51.7 MiB/101.8 MiB]  50% Done 180.3 KiB/s ETA 00:04:45           \r",
      "\\\r",
      "\\ [2/6 files][ 52.4 MiB/101.8 MiB]  51% Done 232.5 KiB/s ETA 00:03:37           \r",
      "|\r",
      "/\r",
      "/ [2/6 files][ 53.2 MiB/101.8 MiB]  52% Done 293.9 KiB/s ETA 00:02:49           \r",
      "-\r",
      "\\\r",
      "\\ [2/6 files][ 53.7 MiB/101.8 MiB]  52% Done 213.2 KiB/s ETA 00:03:51           \r",
      "|\r",
      "/\r",
      "/ [2/6 files][ 54.3 MiB/101.8 MiB]  53% Done 285.6 KiB/s ETA 00:02:50           \r",
      "/ [2/6 files][ 54.5 MiB/101.8 MiB]  53% Done 184.7 KiB/s ETA 00:04:22           \r",
      "-\r",
      "\\\r",
      "\\ [2/6 files][ 55.3 MiB/101.8 MiB]  54% Done 213.7 KiB/s ETA 00:03:43           \r",
      "|\r",
      "/\r",
      "/ [2/6 files][ 55.8 MiB/101.8 MiB]  54% Done 278.9 KiB/s ETA 00:02:49           \r",
      "/ [2/6 files][ 56.3 MiB/101.8 MiB]  55% Done 240.2 KiB/s ETA 00:03:14           \r",
      "-\r",
      "\\\r",
      "\\ [2/6 files][ 56.8 MiB/101.8 MiB]  55% Done 264.1 KiB/s ETA 00:02:54           \r",
      "|\r",
      "| [2/6 files][ 57.3 MiB/101.8 MiB]  56% Done 215.1 KiB/s ETA 00:03:32           \r",
      "/\r",
      "/ [2/6 files][ 57.9 MiB/101.8 MiB]  56% Done 168.5 KiB/s ETA 00:04:27           \r",
      "-\r",
      "\\\r",
      "\\ [2/6 files][ 58.6 MiB/101.8 MiB]  57% Done 220.0 KiB/s ETA 00:03:21           \r",
      "|\r",
      "| [2/6 files][ 59.1 MiB/101.8 MiB]  58% Done 162.4 KiB/s ETA 00:04:29           \r",
      "/\r",
      "-\r",
      "- [2/6 files][ 59.7 MiB/101.8 MiB]  58% Done 228.7 KiB/s ETA 00:03:09           \r",
      "\\\r",
      "|\r",
      "| [2/6 files][ 60.2 MiB/101.8 MiB]  59% Done 182.1 KiB/s ETA 00:03:54           \r",
      "/\r",
      "/ [2/6 files][ 60.7 MiB/101.8 MiB]  59% Done 180.2 KiB/s ETA 00:03:53           \r",
      "-\r",
      "- [2/6 files][ 61.2 MiB/101.8 MiB]  60% Done 294.5 KiB/s ETA 00:02:21           \r",
      "\\\r",
      "\\ [2/6 files][ 61.5 MiB/101.8 MiB]  60% Done 187.0 KiB/s ETA 00:03:41           \r",
      "|\r",
      "| [2/6 files][ 62.2 MiB/101.8 MiB]  61% Done 354.6 KiB/s ETA 00:01:54           \r",
      "/\r",
      "/ [2/6 files][ 62.5 MiB/101.8 MiB]  61% Done 239.3 KiB/s ETA 00:02:48           \r",
      "-\r",
      "- [2/6 files][ 63.0 MiB/101.8 MiB]  61% Done 196.9 KiB/s ETA 00:03:22           \r",
      "\\\r",
      "\\ [2/6 files][ 63.5 MiB/101.8 MiB]  62% Done 264.4 KiB/s ETA 00:02:28           \r",
      "|\r",
      "| [2/6 files][ 63.8 MiB/101.8 MiB]  62% Done 215.6 KiB/s ETA 00:03:00           \r",
      "/\r",
      "/ [2/6 files][ 64.0 MiB/101.8 MiB]  62% Done 213.3 KiB/s ETA 00:03:01           \r",
      "-\r",
      "- [2/6 files][ 64.6 MiB/101.8 MiB]  63% Done 227.1 KiB/s ETA 00:02:48           \r",
      "\\\r",
      "\\ [2/6 files][ 64.8 MiB/101.8 MiB]  63% Done 205.0 KiB/s ETA 00:03:05           \r",
      "|\r",
      "| [2/6 files][ 65.6 MiB/101.8 MiB]  64% Done 211.7 KiB/s ETA 00:02:55           \r",
      "/\r",
      "-\r",
      "- [2/6 files][ 66.1 MiB/101.8 MiB]  64% Done 270.1 KiB/s ETA 00:02:15           \r",
      "\\\r",
      "\\ [2/6 files][ 66.4 MiB/101.8 MiB]  65% Done 237.1 KiB/s ETA 00:02:33           \r",
      "|\r",
      "/\r",
      "/ [2/6 files][ 66.9 MiB/101.8 MiB]  65% Done 284.4 KiB/s ETA 00:02:06           \r",
      "/ [2/6 files][ 67.1 MiB/101.8 MiB]  65% Done 171.8 KiB/s ETA 00:03:26           \r",
      "-\r",
      "- [2/6 files][ 67.7 MiB/101.8 MiB]  66% Done 167.6 KiB/s ETA 00:03:29           \r",
      "\\\r",
      "\\ [2/6 files][ 68.2 MiB/101.8 MiB]  66% Done 268.9 KiB/s ETA 00:02:08           \r",
      "|\r",
      "/\r",
      "/ [2/6 files][ 68.7 MiB/101.8 MiB]  67% Done 221.8 KiB/s ETA 00:02:33           \r",
      "-\r",
      "- [2/6 files][ 68.9 MiB/101.8 MiB]  67% Done 186.8 KiB/s ETA 00:03:00           \r",
      "\\\r",
      "|\r",
      "| [2/6 files][ 69.7 MiB/101.8 MiB]  68% Done 258.2 KiB/s ETA 00:02:07           \r",
      "/\r",
      "/ [2/6 files][ 70.0 MiB/101.8 MiB]  68% Done 295.2 KiB/s ETA 00:01:50           \r",
      "-\r",
      "- [2/6 files][ 70.2 MiB/101.8 MiB]  69% Done 230.4 KiB/s ETA 00:02:20           \r",
      "\\\r",
      "\\ [2/6 files][ 70.5 MiB/101.8 MiB]  69% Done 196.7 KiB/s ETA 00:02:43           \r",
      "|\r",
      "| [2/6 files][ 70.8 MiB/101.8 MiB]  69% Done 180.4 KiB/s ETA 00:02:56           \r",
      "/\r",
      "/ [2/6 files][ 71.3 MiB/101.8 MiB]  70% Done 261.8 KiB/s ETA 00:01:59           \r",
      "-\r",
      "- [2/6 files][ 71.5 MiB/101.8 MiB]  70% Done 250.5 KiB/s ETA 00:02:04           \r",
      "\\\r",
      "\\ [2/6 files][ 71.8 MiB/101.8 MiB]  70% Done 168.1 KiB/s ETA 00:03:03           \r",
      "|\r",
      "/\r",
      "/ [2/6 files][ 72.6 MiB/101.8 MiB]  71% Done 238.1 KiB/s ETA 00:02:06           \r",
      "-\r",
      "- [2/6 files][ 72.8 MiB/101.8 MiB]  71% Done 392.6 KiB/s ETA 00:01:16           \r",
      "\\\r",
      "\\ [2/6 files][ 73.1 MiB/101.8 MiB]  71% Done 292.0 KiB/s ETA 00:01:41           \r",
      "|\r",
      "| [2/6 files][ 73.3 MiB/101.8 MiB]  72% Done 122.0 KiB/s ETA 00:03:59           \r",
      "/\r",
      "-\r",
      "- [2/6 files][ 73.8 MiB/101.8 MiB]  72% Done 217.8 KiB/s ETA 00:02:11           \r",
      "\\\r",
      "\\ [2/6 files][ 74.4 MiB/101.8 MiB]  73% Done 247.7 KiB/s ETA 00:01:53           \r",
      "|\r",
      "| [2/6 files][ 74.9 MiB/101.8 MiB]  73% Done 282.9 KiB/s ETA 00:01:37           \r",
      "/\r",
      "/ [2/6 files][ 75.1 MiB/101.8 MiB]  73% Done 199.4 KiB/s ETA 00:02:17           \r",
      "-\r",
      "\\\r",
      "\\ [2/6 files][ 75.6 MiB/101.8 MiB]  74% Done 223.6 KiB/s ETA 00:02:00           \r",
      "\\ [2/6 files][ 75.9 MiB/101.8 MiB]  74% Done 324.2 KiB/s ETA 00:01:22           \r",
      "|\r",
      "| [2/6 files][ 76.2 MiB/101.8 MiB]  74% Done 219.1 KiB/s ETA 00:02:00           \r",
      "/\r",
      "/ [2/6 files][ 76.4 MiB/101.8 MiB]  75% Done 164.2 KiB/s ETA 00:02:38           \r",
      "-\r",
      "\\\r",
      "\\ [2/6 files][ 76.9 MiB/101.8 MiB]  75% Done 253.2 KiB/s ETA 00:01:40           \r",
      "|\r",
      "| [2/6 files][ 77.2 MiB/101.8 MiB]  75% Done 221.1 KiB/s ETA 00:01:54           \r",
      "/\r",
      "/ [2/6 files][ 77.7 MiB/101.8 MiB]  76% Done 146.0 KiB/s ETA 00:02:49           \r",
      "-\r",
      "\\\r",
      "\\ [2/6 files][ 78.5 MiB/101.8 MiB]  77% Done 424.5 KiB/s ETA 00:00:56           \r",
      "|\r",
      "| [2/6 files][ 79.0 MiB/101.8 MiB]  77% Done 226.1 KiB/s ETA 00:01:43           \r",
      "/\r",
      "-\r",
      "- [2/6 files][ 79.8 MiB/101.8 MiB]  78% Done 312.1 KiB/s ETA 00:01:12           \r",
      "\\\r",
      "\\ [2/6 files][ 80.0 MiB/101.8 MiB]  78% Done 287.8 KiB/s ETA 00:01:17           \r",
      "|\r",
      "| [2/6 files][ 80.3 MiB/101.8 MiB]  78% Done 180.6 KiB/s ETA 00:02:02           \r",
      "/\r",
      "-\r",
      "- [2/6 files][ 80.8 MiB/101.8 MiB]  79% Done 225.5 KiB/s ETA 00:01:35           \r",
      "\\\r",
      "|\r",
      "| [2/6 files][ 81.3 MiB/101.8 MiB]  79% Done 160.0 KiB/s ETA 00:02:11           \r",
      "/\r",
      "-\r",
      "- [2/6 files][ 82.1 MiB/101.8 MiB]  80% Done 379.5 KiB/s ETA 00:00:53           \r",
      "\\\r",
      "\\ [2/6 files][ 82.4 MiB/101.8 MiB]  80% Done 249.4 KiB/s ETA 00:01:20           \r",
      "|\r",
      "| [2/6 files][ 82.6 MiB/101.8 MiB]  81% Done 137.2 KiB/s ETA 00:02:23           \r",
      "/\r",
      "/ [2/6 files][ 83.4 MiB/101.8 MiB]  81% Done 299.5 KiB/s ETA 00:01:03           \r",
      "-\r",
      "- [2/6 files][ 83.6 MiB/101.8 MiB]  82% Done 281.1 KiB/s ETA 00:01:06           \r",
      "\\\r",
      "|\r",
      "| [2/6 files][ 84.2 MiB/101.8 MiB]  82% Done 358.5 KiB/s ETA 00:00:50           \r",
      "/\r",
      "/ [2/6 files][ 84.4 MiB/101.8 MiB]  82% Done 276.3 KiB/s ETA 00:01:04           \r",
      "-\r",
      "- [2/6 files][ 84.9 MiB/101.8 MiB]  83% Done 320.2 KiB/s ETA 00:00:54           \r",
      "\\\r",
      "\\ [2/6 files][ 85.2 MiB/101.8 MiB]  83% Done 160.0 KiB/s ETA 00:01:46           \r",
      "|\r",
      "| [2/6 files][ 85.7 MiB/101.8 MiB]  84% Done 170.9 KiB/s ETA 00:01:36           \r",
      "/\r",
      "/ [2/6 files][ 86.0 MiB/101.8 MiB]  84% Done 259.4 KiB/s ETA 00:01:02           \r",
      "-\r",
      "- [2/6 files][ 86.5 MiB/101.8 MiB]  84% Done 169.5 KiB/s ETA 00:01:32           \r",
      "\\\r",
      "|\r",
      "| [2/6 files][ 87.0 MiB/101.8 MiB]  85% Done 256.4 KiB/s ETA 00:00:59           \r",
      "/\r",
      "/ [2/6 files][ 87.3 MiB/101.8 MiB]  85% Done 287.0 KiB/s ETA 00:00:52           \r",
      "-\r",
      "- [2/6 files][ 87.5 MiB/101.8 MiB]  85% Done 159.2 KiB/s ETA 00:01:32           \r",
      "\\\r",
      "|\r",
      "| [2/6 files][ 88.3 MiB/101.8 MiB]  86% Done 198.7 KiB/s ETA 00:01:10           \r",
      "/\r",
      "/ [2/6 files][ 88.9 MiB/101.8 MiB]  87% Done 191.1 KiB/s ETA 00:01:09           \r",
      "-\r",
      "- [3/6 files][ 88.9 MiB/101.8 MiB]  87% Done 190.1 KiB/s ETA 00:01:09           \r",
      "\\\r",
      "|\r",
      "| [3/6 files][ 89.7 MiB/101.8 MiB]  88% Done 239.0 KiB/s ETA 00:00:52           \r",
      "/\r",
      "-\r",
      "- [3/6 files][ 90.2 MiB/101.8 MiB]  88% Done 301.1 KiB/s ETA 00:00:39           \r",
      "\\\r",
      "\\ [3/6 files][ 90.5 MiB/101.8 MiB]  88% Done 184.9 KiB/s ETA 00:01:03           \r",
      "|\r",
      "| [3/6 files][ 90.7 MiB/101.8 MiB]  89% Done 167.1 KiB/s ETA 00:01:08           \r",
      "/\r",
      "/ [3/6 files][ 91.2 MiB/101.8 MiB]  89% Done 228.7 KiB/s ETA 00:00:47           \r",
      "-\r",
      "- [3/6 files][ 91.5 MiB/101.8 MiB]  89% Done 230.8 KiB/s ETA 00:00:46           \r",
      "\\\r",
      "|\r",
      "| [3/6 files][ 92.0 MiB/101.8 MiB]  90% Done 186.6 KiB/s ETA 00:00:54           \r",
      "/\r",
      "-\r",
      "- [3/6 files][ 92.8 MiB/101.8 MiB]  91% Done 235.4 KiB/s ETA 00:00:39           \r",
      "\\\r",
      "\\ [3/6 files][ 93.1 MiB/101.8 MiB]  91% Done 173.2 KiB/s ETA 00:00:52           \r",
      "|\r",
      "| [3/6 files][ 93.8 MiB/101.8 MiB]  92% Done 290.6 KiB/s ETA 00:00:28           \r",
      "/\r",
      "/ [3/6 files][ 94.3 MiB/101.8 MiB]  92% Done 273.1 KiB/s ETA 00:00:28           \r",
      "-\r",
      "\\\r",
      "\\ [3/6 files][ 94.9 MiB/101.8 MiB]  93% Done 248.2 KiB/s ETA 00:00:29           \r",
      "|\r",
      "| [3/6 files][ 95.1 MiB/101.8 MiB]  93% Done 208.7 KiB/s ETA 00:00:33           \r",
      "/\r",
      "-\r",
      "- [3/6 files][ 95.6 MiB/101.8 MiB]  93% Done 196.5 KiB/s ETA 00:00:32           \r",
      "\\\r",
      "|\r",
      "| [3/6 files][ 96.1 MiB/101.8 MiB]  94% Done 238.1 KiB/s ETA 00:00:24           \r",
      "/\r",
      "/ [3/6 files][ 96.4 MiB/101.8 MiB]  94% Done 210.8 KiB/s ETA 00:00:26           \r",
      "-\r",
      "- [3/6 files][ 96.9 MiB/101.8 MiB]  95% Done 219.5 KiB/s ETA 00:00:23           \r",
      "\\\r",
      "|\r",
      "| [3/6 files][ 97.4 MiB/101.8 MiB]  95% Done 249.8 KiB/s ETA 00:00:18           \r",
      "/\r",
      "/ [4/6 files][ 97.4 MiB/101.8 MiB]  95% Done 212.9 KiB/s ETA 00:00:21           \r",
      "-\r",
      "- [4/6 files][ 97.9 MiB/101.8 MiB]  96% Done 206.8 KiB/s ETA 00:00:19           \r",
      "\\\r",
      "\\ [4/6 files][ 98.1 MiB/101.8 MiB]  96% Done 186.0 KiB/s ETA 00:00:20           \r",
      "|\r",
      "/\r",
      "/ [4/6 files][ 98.7 MiB/101.8 MiB]  96% Done 224.2 KiB/s ETA 00:00:14           \r",
      "-\r",
      "- [5/6 files][ 98.7 MiB/101.8 MiB]  96% Done 205.0 KiB/s ETA 00:00:15           \r",
      "\\\r",
      "\\ [5/6 files][ 99.2 MiB/101.8 MiB]  97% Done 238.5 KiB/s ETA 00:00:11           \r",
      "|\r",
      "| [5/6 files][ 99.5 MiB/101.8 MiB]  97% Done 214.3 KiB/s ETA 00:00:11           \r",
      "/\r",
      "/ [5/6 files][ 99.7 MiB/101.8 MiB]  98% Done 212.6 KiB/s ETA 00:00:10           \r",
      "-\r",
      "- [5/6 files][100.0 MiB/101.8 MiB]  98% Done 211.5 KiB/s ETA 00:00:09           \r",
      "\\\r",
      "\\ [5/6 files][100.3 MiB/101.8 MiB]  98% Done 201.1 KiB/s ETA 00:00:08           \r",
      "|\r",
      "| [5/6 files][100.5 MiB/101.8 MiB]  98% Done 201.1 KiB/s ETA 00:00:06           \r",
      "/\r",
      "/ [5/6 files][100.8 MiB/101.8 MiB]  99% Done 206.9 KiB/s ETA 00:00:05           \r",
      "-\r",
      "- [5/6 files][101.0 MiB/101.8 MiB]  99% Done 216.0 KiB/s ETA 00:00:04           \r",
      "\\\r",
      "\\ [5/6 files][101.3 MiB/101.8 MiB]  99% Done 216.1 KiB/s ETA 00:00:02           \r",
      "|\r",
      "| [5/6 files][101.6 MiB/101.8 MiB]  99% Done 219.7 KiB/s ETA 00:00:01           \r",
      "/\r",
      "/ [5/6 files][101.8 MiB/101.8 MiB]  99% Done 196.0 KiB/s ETA 00:00:00           \r",
      "-\r",
      "- [6/6 files][101.8 MiB/101.8 MiB] 100% Done 165.2 KiB/s ETA 00:00:00           \r",
      "\\\r\n",
      "Operation completed over 6 objects/101.8 MiB.                                    \n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "saved_models_base=models/mnist/cnn_classifier/export/\n",
    "saved_model_dir=${saved_models_base}$(ls ${saved_models_base} | tail -n 1)\n",
    "\n",
    "echo ${saved_model_dir}\n",
    "\n",
    "gsutil -m cp -r ${saved_model_dir} gs://${BUCKET}/tf-model-optimisation/original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "models/mnist/cnn_classifier/export/1536860075/optimised\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Copying file://models/mnist/cnn_classifier/export/1536860075/optimised/saved_model.pb [Content-Type=application/octet-stream]...\n",
      "/ [0/1 files][    0.0 B/ 24.8 MiB]   0% Done                                    \r",
      "-\r",
      "- [0/1 files][528.0 KiB/ 24.8 MiB]   2% Done                                    \r",
      "\\\r",
      "\\ [0/1 files][  1.3 MiB/ 24.8 MiB]   5% Done                                    \r",
      "|\r",
      "| [0/1 files][  1.6 MiB/ 24.8 MiB]   6% Done                                    \r",
      "/\r",
      "/ [0/1 files][  2.1 MiB/ 24.8 MiB]   8% Done                                    \r",
      "-\r",
      "\\\r",
      "\\ [0/1 files][  2.6 MiB/ 24.8 MiB]  10% Done                                    \r",
      "\\ [0/1 files][  2.8 MiB/ 24.8 MiB]  11% Done 339.1 KiB/s ETA 00:01:06           \r",
      "|\r",
      "| [0/1 files][  3.1 MiB/ 24.8 MiB]  12% Done 158.7 KiB/s ETA 00:02:20           \r",
      "/\r",
      "/ [0/1 files][  3.9 MiB/ 24.8 MiB]  15% Done 184.8 KiB/s ETA 00:01:56           \r",
      "-\r",
      "- [0/1 files][  4.6 MiB/ 24.8 MiB]  18% Done 213.1 KiB/s ETA 00:01:37           \r",
      "\\\r",
      "\\ [0/1 files][  4.9 MiB/ 24.8 MiB]  19% Done 505.2 KiB/s ETA 00:00:40           \r",
      "|\r",
      "| [0/1 files][  5.2 MiB/ 24.8 MiB]  20% Done 109.0 KiB/s ETA 00:03:05           \r",
      "/\r",
      "-\r",
      "- [0/1 files][  5.9 MiB/ 24.8 MiB]  23% Done 453.4 KiB/s ETA 00:00:43           \r",
      "\\\r",
      "\\ [0/1 files][  6.2 MiB/ 24.8 MiB]  24% Done 373.4 KiB/s ETA 00:00:51           \r",
      "|\r",
      "| [0/1 files][  6.5 MiB/ 24.8 MiB]  25% Done 115.7 KiB/s ETA 00:02:43           \r",
      "/\r",
      "-\r",
      "- [0/1 files][  7.0 MiB/ 24.8 MiB]  28% Done 171.8 KiB/s ETA 00:01:47           \r",
      "\\\r",
      "\\ [0/1 files][  7.5 MiB/ 24.8 MiB]  30% Done 362.7 KiB/s ETA 00:00:49           \r",
      "|\r",
      "| [0/1 files][  7.7 MiB/ 24.8 MiB]  31% Done 116.4 KiB/s ETA 00:02:30           \r",
      "/\r",
      "/ [0/1 files][  8.3 MiB/ 24.8 MiB]  33% Done 169.3 KiB/s ETA 00:01:40           \r",
      "-\r",
      "- [0/1 files][  8.5 MiB/ 24.8 MiB]  34% Done 365.1 KiB/s ETA 00:00:46           \r",
      "\\\r",
      "\\ [0/1 files][  8.8 MiB/ 24.8 MiB]  35% Done 216.1 KiB/s ETA 00:01:16           \r",
      "|\r",
      "| [0/1 files][  9.3 MiB/ 24.8 MiB]  37% Done 213.1 KiB/s ETA 00:01:15           \r",
      "/\r",
      "-\r",
      "- [0/1 files][  9.8 MiB/ 24.8 MiB]  39% Done 320.6 KiB/s ETA 00:00:48           \r",
      "\\\r",
      "\\ [0/1 files][ 10.1 MiB/ 24.8 MiB]  40% Done 298.5 KiB/s ETA 00:00:51           \r",
      "|\r",
      "| [0/1 files][ 10.3 MiB/ 24.8 MiB]  41% Done 244.6 KiB/s ETA 00:01:01           \r",
      "/\r",
      "/ [0/1 files][ 10.6 MiB/ 24.8 MiB]  42% Done 233.4 KiB/s ETA 00:01:03           \r",
      "-\r",
      "- [0/1 files][ 10.8 MiB/ 24.8 MiB]  43% Done 227.8 KiB/s ETA 00:01:03           \r",
      "\\\r",
      "\\ [0/1 files][ 11.1 MiB/ 24.8 MiB]  44% Done 227.9 KiB/s ETA 00:01:02           \r",
      "|\r",
      "| [0/1 files][ 11.3 MiB/ 24.8 MiB]  45% Done 228.2 KiB/s ETA 00:01:01           \r",
      "/\r",
      "-\r",
      "- [0/1 files][ 11.9 MiB/ 24.8 MiB]  47% Done 254.7 KiB/s ETA 00:00:52           \r",
      "\\\r",
      "|\r",
      "| [0/1 files][ 12.4 MiB/ 24.8 MiB]  49% Done 282.0 KiB/s ETA 00:00:45           \r",
      "/\r",
      "-\r",
      "- [0/1 files][ 12.9 MiB/ 24.8 MiB]  51% Done 112.1 KiB/s ETA 00:01:49           \r",
      "\\\r",
      "|\r",
      "| [0/1 files][ 13.7 MiB/ 24.8 MiB]  55% Done 618.5 KiB/s ETA 00:00:18           \r",
      "/\r",
      "/ [0/1 files][ 13.9 MiB/ 24.8 MiB]  56% Done 454.7 KiB/s ETA 00:00:25           \r",
      "-\r",
      "- [0/1 files][ 14.2 MiB/ 24.8 MiB]  57% Done 111.9 KiB/s ETA 00:01:38           \r",
      "\\\r",
      "|\r",
      "| [0/1 files][ 14.7 MiB/ 24.8 MiB]  59% Done 167.9 KiB/s ETA 00:01:02           \r",
      "/\r",
      "/ [0/1 files][ 15.2 MiB/ 24.8 MiB]  61% Done 387.8 KiB/s ETA 00:00:25           \r",
      "-\r",
      "- [0/1 files][ 15.5 MiB/ 24.8 MiB]  62% Done 347.5 KiB/s ETA 00:00:28           \r",
      "\\\r",
      "\\ [0/1 files][ 15.7 MiB/ 24.8 MiB]  63% Done 211.7 KiB/s ETA 00:00:44           \r",
      "|\r",
      "/\r",
      "/ [0/1 files][ 16.2 MiB/ 24.8 MiB]  65% Done 196.5 KiB/s ETA 00:00:45           \r",
      "-\r",
      "- [0/1 files][ 16.5 MiB/ 24.8 MiB]  66% Done 282.5 KiB/s ETA 00:00:30           \r",
      "\\\r",
      "\\ [0/1 files][ 16.8 MiB/ 24.8 MiB]  67% Done 269.7 KiB/s ETA 00:00:31           \r",
      "|\r",
      "| [0/1 files][ 17.0 MiB/ 24.8 MiB]  68% Done 238.9 KiB/s ETA 00:00:34           \r",
      "/\r",
      "/ [0/1 files][ 17.3 MiB/ 24.8 MiB]  69% Done 161.5 KiB/s ETA 00:00:48           \r",
      "-\r",
      "- [0/1 files][ 17.8 MiB/ 24.8 MiB]  71% Done 202.8 KiB/s ETA 00:00:36           \r",
      "\\\r",
      "\\ [0/1 files][ 18.1 MiB/ 24.8 MiB]  72% Done 339.0 KiB/s ETA 00:00:21           \r",
      "|\r",
      "| [0/1 files][ 18.3 MiB/ 24.8 MiB]  73% Done 309.2 KiB/s ETA 00:00:22           \r",
      "/\r",
      "-\r",
      "- [0/1 files][ 18.8 MiB/ 24.8 MiB]  75% Done 257.8 KiB/s ETA 00:00:24           \r",
      "\\\r",
      "\\ [0/1 files][ 19.1 MiB/ 24.8 MiB]  76% Done 252.5 KiB/s ETA 00:00:23           \r",
      "|\r",
      "| [0/1 files][ 19.3 MiB/ 24.8 MiB]  77% Done 245.1 KiB/s ETA 00:00:23           \r",
      "/\r",
      "/ [0/1 files][ 19.6 MiB/ 24.8 MiB]  78% Done 159.3 KiB/s ETA 00:00:34           \r",
      "-\r",
      "- [0/1 files][ 20.1 MiB/ 24.8 MiB]  80% Done 149.9 KiB/s ETA 00:00:32           \r",
      "\\\r",
      "|\r",
      "| [0/1 files][ 20.9 MiB/ 24.8 MiB]  84% Done 211.5 KiB/s ETA 00:00:19           \r",
      "/\r",
      "/ [0/1 files][ 21.1 MiB/ 24.8 MiB]  85% Done 261.4 KiB/s ETA 00:00:14           \r",
      "-\r",
      "\\\r",
      "\\ [0/1 files][ 21.7 MiB/ 24.8 MiB]  87% Done 216.6 KiB/s ETA 00:00:15           \r",
      "|\r",
      "| [0/1 files][ 21.9 MiB/ 24.8 MiB]  88% Done 116.1 KiB/s ETA 00:00:26           \r",
      "/\r",
      "/ [0/1 files][ 22.4 MiB/ 24.8 MiB]  90% Done 303.8 KiB/s ETA 00:00:08           \r",
      "-\r",
      "\\\r",
      "\\ [0/1 files][ 23.0 MiB/ 24.8 MiB]  92% Done 264.9 KiB/s ETA 00:00:07           \r",
      "|\r",
      "| [0/1 files][ 23.2 MiB/ 24.8 MiB]  93% Done 218.7 KiB/s ETA 00:00:08           \r",
      "/\r",
      "/ [0/1 files][ 23.5 MiB/ 24.8 MiB]  94% Done 240.2 KiB/s ETA 00:00:06           \r",
      "-\r",
      "- [0/1 files][ 23.7 MiB/ 24.8 MiB]  95% Done 223.0 KiB/s ETA 00:00:05           \r",
      "\\\r",
      "|\r",
      "| [0/1 files][ 24.2 MiB/ 24.8 MiB]  97% Done 237.7 KiB/s ETA 00:00:03           \r",
      "/\r",
      "/ [0/1 files][ 24.5 MiB/ 24.8 MiB]  98% Done 231.0 KiB/s ETA 00:00:02           \r",
      "-\r",
      "- [0/1 files][ 24.8 MiB/ 24.8 MiB]  99% Done 229.7 KiB/s ETA 00:00:00           \r",
      "\\\r",
      "\\ [1/1 files][ 24.8 MiB/ 24.8 MiB] 100% Done 135.4 KiB/s ETA 00:00:00           \r",
      "|\r\n",
      "Operation completed over 1 objects/24.8 MiB.                                     \n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "saved_models_base=models/mnist/cnn_classifier/export/\n",
    "saved_model_dir=${saved_models_base}$(ls ${saved_models_base} | tail -n 1)/optimised\n",
    "\n",
    "echo ${saved_model_dir}\n",
    "\n",
    "gsutil -m cp -r ${saved_model_dir} gs://${BUCKET}/tf-model-optimisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Deploy models to Cloud ML Engine\n",
    "\n",
    "Don't forget to delete the model and the model version if they were previously deployed!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created ml engine model [projects/ksalama-gcp-playground/models/mnist_classifier].\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "echo ${MODEL_NAME}\n",
    "\n",
    "gcloud ml-engine models create ${MODEL_NAME} --regions=${REGION}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Version: v_org** is the original SavedModel (before optimisation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating version (this might take a few minutes)......\n",
      "..............................................................................................................................................................................................................................done.\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "MODEL_VERSION='v_org'\n",
    "MODEL_ORIGIN=gs://${BUCKET}/tf-model-optimisation/original\n",
    "\n",
    "gcloud ml-engine versions create ${MODEL_VERSION}\\\n",
    "            --model=${MODEL_NAME} \\\n",
    "            --origin=${MODEL_ORIGIN} \\\n",
    "            --runtime-version=1.10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Version: v_opt** is the optimised SavedModel (after optimisation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating version (this might take a few minutes)......\n",
      "....................................................................................................................................................done.\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "MODEL_VERSION='v_opt'\n",
    "MODEL_ORIGIN=gs://${BUCKET}/tf-model-optimisation/optimised\n",
    "\n",
    "gcloud ml-engine versions create ${MODEL_VERSION}\\\n",
    "            --model=${MODEL_NAME} \\\n",
    "            --origin=${MODEL_ORIGIN} \\\n",
    "            --runtime-version=1.10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Cloud ML Engine online predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "from googleapiclient import discovery\n",
    "from oauth2client.client import GoogleCredentials\n",
    "\n",
    "credentials = GoogleCredentials.get_application_default()\n",
    "api = discovery.build(\n",
    "    'ml', 'v1', \n",
    "    credentials=credentials, \n",
    "    discoveryServiceUrl='https://storage.googleapis.com/cloud-ml/discovery/ml_v1_discovery.json'\n",
    ")\n",
    "\n",
    "    \n",
    "def predict(version, instances):\n",
    "\n",
    "    request_data = {'instances': instances}\n",
    "\n",
    "    model_url = 'projects/{}/models/{}/versions/{}'.format(PROJECT, MODEL_NAME, version)\n",
    "    response = api.projects().predict(body=request_data, name=model_url).execute()\n",
    "\n",
    "    class_ids = None\n",
    "    \n",
    "    try:\n",
    "        class_ids = [item[\"class_ids\"] for item in response[\"predictions\"]]\n",
    "    except:\n",
    "        print response\n",
    "    \n",
    "    return class_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference_cmle(version, batch=100, repeat=10):\n",
    "    \n",
    "    instances = [\n",
    "            {'input_image': [float(i) for i in list(eval_data[img])] }\n",
    "        for img in range(batch)\n",
    "    ]\n",
    "\n",
    "    #warmup request\n",
    "    predict(version, instances[0])\n",
    "    print 'Warm up request performed!'\n",
    "    print 'Timer started...'\n",
    "    print ''\n",
    "    \n",
    "    time_start = datetime.utcnow() \n",
    "    output = None\n",
    "    \n",
    "    for i in range(repeat):\n",
    "        output = predict(version, instances)\n",
    "    \n",
    "    time_end = datetime.utcnow() \n",
    "\n",
    "    time_elapsed_sec = (time_end - time_start).total_seconds()\n",
    "    \n",
    "    print \"Inference elapsed time: {} seconds\".format(time_elapsed_sec)\n",
    "    print \"\"\n",
    "    \n",
    "    print \"Prediction produced for {} instances batch, repeated {} times\".format(len(output), repeat)\n",
    "    print \"Average latency per batch: {} seconds\".format(time_elapsed_sec/repeat)\n",
    "    print \"\"\n",
    "    \n",
    "    print \"Prediction output for the last instance: {}\".format(output[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warm up request performed!\n",
      "Timer started...\n",
      "\n",
      "Inference elapsed time: 36.793954 seconds\n",
      "\n",
      "Prediction produced for 100 instances batch, repeated 10 times\n",
      "Average latency per batch: 3.6793954 seconds\n",
      "\n",
      "Prediction output for the last instance: [7]\n"
     ]
    }
   ],
   "source": [
    "version='v_org'\n",
    "inference_cmle(version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warm up request performed!\n",
      "Timer started...\n",
      "\n",
      "Inference elapsed time: 34.383098 seconds\n",
      "\n",
      "Prediction produced for 100 instances batch, repeated 10 times\n",
      "Average latency per batch: 3.4383098 seconds\n",
      "\n",
      "Prediction output for the last instance: [7]\n"
     ]
    }
   ],
   "source": [
    "version='v_opt'\n",
    "inference_cmle(version)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Happy serving!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
