{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimising a TensorFlow SavedModel for Serving"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebooks shows how to optimise the TensorFlow exported SavedModel by **shrinking** its size (to have less memory and disk footprints), and **improving** prediction latency. This can be accopmlished by applying the following:\n",
    "* **Freezing**: That is, converting the variables stored in a checkpoint file of the SavedModel into constants stored directly in the model graph.\n",
    "* **Pruning**: That is, stripping unused nodes during the prediction path of the graph, merging duplicate nodes, as well as removing other node ops like summary, identity, etc.\n",
    "* **Quantisation**:  That is, converting any large float Const op into an eight-bit equivalent, followed by a float conversion op so that the result is usable by subsequent nodes.\n",
    "* **Other refinements**: That includes constant folding, batch_norm folding, fusing convolusion, etc.\n",
    "\n",
    "The optimisation operations we apply in this example are from the TensorFlow [Graph Conversion Tool](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/graph_transforms/README.md#fold_constants), which is a c++ command-line tool. We use the Python APIs to call the c++ libraries. \n",
    "\n",
    "The Graph Transform Tool is designed to work on models that are saved as GraphDef files, usually in a binary protobuf format. However, the model exported after training and estimator is in SavedModel format (saved_model.pb file + variables folder with variables.data-* and variables.index files). \n",
    "\n",
    "We need to optimise the mode and keep it the SavedModel format. Thus, the optimisation steps will be:\n",
    "1. Freeze the SavedModel: SavedModel -> GraphDef\n",
    "2. Optimisae the freezed model: GraphDef -> GraphDef\n",
    "3. Convert the optimised freezed model to SavedModel: GraphDef -> SavedModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow : 1.10.0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import data\n",
    "\n",
    "print \"TensorFlow : {}\".format(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Train and Export a TensorFlow DNNClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "CSV_HEADER = ['sepal_length', 'sepal_width',  'petal_length', 'petal_width', 'species']\n",
    "NUMERIC_FEATURE_NAMES = ['sepal_length', 'sepal_width',  'petal_length', 'petal_width']\n",
    "TARGET_NAME = 'species'\n",
    "TARGET_LABELS = ['setosa', 'virginica', 'versicolor']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_input_fn(file_pattern, num_epochs=1, batch_size=50, mode=tf.estimator.ModeKeys.EVAL):\n",
    "    \n",
    "    def _input_fn():\n",
    "        \n",
    "        features = tf.contrib.data.make_csv_dataset(\n",
    "            file_pattern=file_pattern,\n",
    "            column_names=CSV_HEADER,\n",
    "            header=True,\n",
    "            num_epochs=num_epochs,\n",
    "            batch_size=batch_size,\n",
    "            shuffle=True if mode == tf.estimator.ModeKeys.TRAIN else False            \n",
    "        ).make_one_shot_iterator().get_next()\n",
    "\n",
    "        target = features.pop(TARGET_NAME)\n",
    "        return features, target\n",
    "    \n",
    "    return _input_fn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Serving Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_serving_fn():\n",
    "    \n",
    "    inputs = {\n",
    "        feature_name: tf.placeholder(name=feature_name, shape=[None], dtype=tf.float32)\n",
    "        for feature_name in NUMERIC_FEATURE_NAMES\n",
    "    }\n",
    "    serving_fn = tf.estimator.export.build_raw_serving_input_receiver_fn(inputs)\n",
    "    \n",
    "    return serving_fn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_estimator(hparams, run_config):\n",
    "    \n",
    "    feature_columns = [\n",
    "        tf.feature_column.numeric_column(feature_name) \n",
    "        for feature_name in NUMERIC_FEATURE_NAMES\n",
    "    ]\n",
    "    \n",
    "    estimator = tf.estimator.DNNClassifier(\n",
    "        n_classes=len(TARGET_LABELS),\n",
    "        label_vocabulary=TARGET_LABELS, \n",
    "        feature_columns= feature_columns,\n",
    "        hidden_units= hparams.hidden_units,\n",
    "        config=run_config\n",
    "    )\n",
    "    \n",
    "    return estimator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and Evaluate Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment(hparam, run_config):\n",
    "    \n",
    "    train_spec = tf.estimator.TrainSpec(\n",
    "        input_fn = make_input_fn(\n",
    "            hparam.train_files,\n",
    "            num_epochs=hparams.num_epochs,\n",
    "            batch_size=hparams.batch_size,\n",
    "            mode=tf.estimator.ModeKeys.TRAIN\n",
    "        ),\n",
    "        max_steps=hparams.max_steps\n",
    "    )\n",
    "\n",
    "    eval_spec = tf.estimator.EvalSpec(\n",
    "        input_fn = make_input_fn(\n",
    "            hparam.eval_files,\n",
    "            batch_size=hparams.batch_size\n",
    "        ),\n",
    "        exporters=[tf.estimator.LatestExporter(\n",
    "            name=\"estimate\", \n",
    "            serving_input_receiver_fn=make_serving_fn(),\n",
    "            exports_to_keep=1,\n",
    "            as_text=False)],\n",
    "        steps=hparams.eval_steps,\n",
    "        throttle_secs=hparams.eval_throttle_secs,\n",
    "        start_delay_secs=1\n",
    "    )\n",
    "\n",
    "    print(\"Removing previous artifacts...\")\n",
    "    if tf.gfile.Exists(run_config.model_dir):\n",
    "        tf.gfile.DeleteRecursively(run_config.model_dir)\n",
    "\n",
    "    tf.logging.set_verbosity(tf.logging.INFO)\n",
    "\n",
    "    time_start = datetime.utcnow() \n",
    "    print(\"Experiment started at {}\".format(time_start.strftime(\"%H:%M:%S\")))\n",
    "    print(\".......................................\") \n",
    "\n",
    "    estimator = create_estimator(hparams, run_config)\n",
    "\n",
    "    tf.estimator.train_and_evaluate(\n",
    "        estimator=estimator,\n",
    "        train_spec=train_spec, \n",
    "        eval_spec=eval_spec\n",
    "    )\n",
    "\n",
    "    time_end = datetime.utcnow() \n",
    "    print(\".......................................\")\n",
    "    print(\"Experiment finished at {}\".format(time_end.strftime(\"%H:%M:%S\")))\n",
    "    print(\"\")\n",
    "    time_elapsed = time_end - time_start\n",
    "    print(\"Experiment elapsed time: {} seconds\".format(time_elapsed.total_seconds()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_DATA_FILES = 'iris/data/train-*.csv'\n",
    "EVAL_DATA_FILES = 'iris/data/train-*.csv'\n",
    "MODELS_LOCATION = 'iris/models'\n",
    "MODEL_NAME = 'iris_classifier'\n",
    "MODEL_DIR = os.path.join(MODELS_LOCATION, MODEL_NAME)\n",
    "\n",
    "hparams  = tf.contrib.training.HParams(\n",
    "    train_files=TRAIN_DATA_FILES,\n",
    "    eval_files=EVAL_DATA_FILES,\n",
    "    num_epochs=1000, \n",
    "    batch_size=50,\n",
    "    hidden_units=[512, 512, 512, 512],\n",
    "    max_steps=None,\n",
    "    eval_throttle_secs=1,\n",
    "    eval_steps=None\n",
    ")\n",
    "\n",
    "run_config = tf.estimator.RunConfig(\n",
    "    tf_random_seed=19830610,\n",
    "    save_checkpoints_steps=1000,\n",
    "    keep_checkpoint_max=3,\n",
    "    model_dir=MODEL_DIR\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing previous artifacts...\n",
      "Experiment started at 13:46:18\n",
      ".......................................\n",
      "INFO:tensorflow:Using config: {'_save_checkpoints_secs': None, '_global_id_in_cluster': 0, '_session_config': None, '_keep_checkpoint_max': 3, '_tf_random_seed': 19830610, '_task_type': 'worker', '_train_distribute': None, '_is_chief': True, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x11a1f2110>, '_model_dir': 'iris/models/iris_classifier', '_num_worker_replicas': 1, '_task_id': 0, '_log_step_count_steps': 100, '_master': '', '_save_checkpoints_steps': 1000, '_keep_checkpoint_every_n_hours': 10000, '_evaluation_master': '', '_service': None, '_device_fn': None, '_save_summary_steps': 100, '_num_ps_replicas': 0}\n",
      "INFO:tensorflow:Running training and evaluation locally (non-distributed).\n",
      "INFO:tensorflow:Start train and evaluate loop. The evaluate will happen after every checkpoint. Checkpoint frequency is determined based on RunConfig arguments: save_checkpoints_steps 1000 or save_checkpoints_secs None.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 0 into iris/models/iris_classifier/model.ckpt.\n",
      "INFO:tensorflow:loss = 57.043484, step = 1\n",
      "INFO:tensorflow:global_step/sec: 88.0245\n",
      "INFO:tensorflow:loss = 13.818828, step = 101 (1.138 sec)\n",
      "INFO:tensorflow:global_step/sec: 105.955\n",
      "INFO:tensorflow:loss = 5.37489, step = 201 (0.943 sec)\n",
      "INFO:tensorflow:global_step/sec: 117.644\n",
      "INFO:tensorflow:loss = 2.2458813, step = 301 (0.850 sec)\n",
      "INFO:tensorflow:global_step/sec: 114.196\n",
      "INFO:tensorflow:loss = 3.0473716, step = 401 (0.876 sec)\n",
      "INFO:tensorflow:global_step/sec: 115.81\n",
      "INFO:tensorflow:loss = 5.8213706, step = 501 (0.863 sec)\n",
      "INFO:tensorflow:global_step/sec: 107.384\n",
      "INFO:tensorflow:loss = 1.65476, step = 601 (0.930 sec)\n",
      "INFO:tensorflow:global_step/sec: 118.06\n",
      "INFO:tensorflow:loss = 3.2814267, step = 701 (0.847 sec)\n",
      "INFO:tensorflow:global_step/sec: 114.721\n",
      "INFO:tensorflow:loss = 6.265688, step = 801 (0.876 sec)\n",
      "INFO:tensorflow:global_step/sec: 115.316\n",
      "INFO:tensorflow:loss = 0.49367592, step = 901 (0.863 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 1000 into iris/models/iris_classifier/model.ckpt.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2018-08-13-13:46:29\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from iris/models/iris_classifier/model.ckpt-1000\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Finished evaluation at 2018-08-13-13:46:30\n",
      "INFO:tensorflow:Saving dict for global step 1000: accuracy = 0.96666664, average_loss = 0.060419902, global_step = 1000, loss = 3.0209951\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 1000: iris/models/iris_classifier/model.ckpt-1000\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Eval: None\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Classify: None\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Regress: None\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Predict: ['predict']\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Train: None\n",
      "INFO:tensorflow:Signatures EXCLUDED from export because they cannot be be served via TensorFlow Serving APIs:\n",
      "INFO:tensorflow:'serving_default' : Classification input must be a single string Tensor; got {'sepal_width': <tf.Tensor 'sepal_width:0' shape=(?,) dtype=float32>, 'petal_width': <tf.Tensor 'petal_width:0' shape=(?,) dtype=float32>, 'sepal_length': <tf.Tensor 'sepal_length:0' shape=(?,) dtype=float32>, 'petal_length': <tf.Tensor 'petal_length:0' shape=(?,) dtype=float32>}\n",
      "INFO:tensorflow:'classification' : Classification input must be a single string Tensor; got {'sepal_width': <tf.Tensor 'sepal_width:0' shape=(?,) dtype=float32>, 'petal_width': <tf.Tensor 'petal_width:0' shape=(?,) dtype=float32>, 'sepal_length': <tf.Tensor 'sepal_length:0' shape=(?,) dtype=float32>, 'petal_length': <tf.Tensor 'petal_length:0' shape=(?,) dtype=float32>}\n",
      "WARNING:tensorflow:Export includes no default signature!\n",
      "INFO:tensorflow:Restoring parameters from iris/models/iris_classifier/model.ckpt-1000\n",
      "INFO:tensorflow:Assets added to graph.\n",
      "INFO:tensorflow:No assets to write.\n",
      "INFO:tensorflow:SavedModel written to: iris/models/iris_classifier/export/estimate/temp-1534167990/saved_model.pb\n",
      "INFO:tensorflow:global_step/sec: 40.8028\n",
      "INFO:tensorflow:loss = 7.022607, step = 1001 (2.452 sec)\n",
      "INFO:tensorflow:global_step/sec: 117.685\n",
      "INFO:tensorflow:loss = 2.6916559, step = 1101 (0.848 sec)\n",
      "INFO:tensorflow:global_step/sec: 120.037\n",
      "INFO:tensorflow:loss = 1.4519235, step = 1201 (0.833 sec)\n",
      "INFO:tensorflow:global_step/sec: 110.884\n",
      "INFO:tensorflow:loss = 2.0469675, step = 1301 (0.903 sec)\n",
      "INFO:tensorflow:global_step/sec: 113.806\n",
      "INFO:tensorflow:loss = 3.64328, step = 1401 (0.878 sec)\n",
      "INFO:tensorflow:global_step/sec: 108.472\n",
      "INFO:tensorflow:loss = 0.60582185, step = 1501 (0.922 sec)\n",
      "INFO:tensorflow:global_step/sec: 108.877\n",
      "INFO:tensorflow:loss = 0.7663758, step = 1601 (0.919 sec)\n",
      "INFO:tensorflow:global_step/sec: 121.018\n",
      "INFO:tensorflow:loss = 0.56999964, step = 1701 (0.826 sec)\n",
      "INFO:tensorflow:global_step/sec: 113.214\n",
      "INFO:tensorflow:loss = 2.7651718, step = 1801 (0.883 sec)\n",
      "INFO:tensorflow:global_step/sec: 110.859\n",
      "INFO:tensorflow:loss = 2.6943533, step = 1901 (0.902 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 2000 into iris/models/iris_classifier/model.ckpt.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2018-08-13-13:46:40\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from iris/models/iris_classifier/model.ckpt-2000\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Finished evaluation at 2018-08-13-13:46:40\n",
      "INFO:tensorflow:Saving dict for global step 2000: accuracy = 0.9866667, average_loss = 0.040331095, global_step = 2000, loss = 2.0165548\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 2000: iris/models/iris_classifier/model.ckpt-2000\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Eval: None\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Classify: None\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Regress: None\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Predict: ['predict']\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Train: None\n",
      "INFO:tensorflow:Signatures EXCLUDED from export because they cannot be be served via TensorFlow Serving APIs:\n",
      "INFO:tensorflow:'serving_default' : Classification input must be a single string Tensor; got {'sepal_width': <tf.Tensor 'sepal_width:0' shape=(?,) dtype=float32>, 'petal_width': <tf.Tensor 'petal_width:0' shape=(?,) dtype=float32>, 'sepal_length': <tf.Tensor 'sepal_length:0' shape=(?,) dtype=float32>, 'petal_length': <tf.Tensor 'petal_length:0' shape=(?,) dtype=float32>}\n",
      "INFO:tensorflow:'classification' : Classification input must be a single string Tensor; got {'sepal_width': <tf.Tensor 'sepal_width:0' shape=(?,) dtype=float32>, 'petal_width': <tf.Tensor 'petal_width:0' shape=(?,) dtype=float32>, 'sepal_length': <tf.Tensor 'sepal_length:0' shape=(?,) dtype=float32>, 'petal_length': <tf.Tensor 'petal_length:0' shape=(?,) dtype=float32>}\n",
      "WARNING:tensorflow:Export includes no default signature!\n",
      "INFO:tensorflow:Restoring parameters from iris/models/iris_classifier/model.ckpt-2000\n",
      "INFO:tensorflow:Assets added to graph.\n",
      "INFO:tensorflow:No assets to write.\n",
      "INFO:tensorflow:SavedModel written to: iris/models/iris_classifier/export/estimate/temp-1534168000/saved_model.pb\n",
      "INFO:tensorflow:global_step/sec: 41.5349\n",
      "INFO:tensorflow:loss = 1.0251423, step = 2001 (2.408 sec)\n",
      "INFO:tensorflow:global_step/sec: 97.5083\n",
      "INFO:tensorflow:loss = 1.1533114, step = 2101 (1.025 sec)\n",
      "INFO:tensorflow:global_step/sec: 95.5923\n",
      "INFO:tensorflow:loss = 3.62031, step = 2201 (1.047 sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 97.9638\n",
      "INFO:tensorflow:loss = 5.948579, step = 2301 (1.020 sec)\n",
      "INFO:tensorflow:global_step/sec: 100.675\n",
      "INFO:tensorflow:loss = 0.7604561, step = 2401 (0.993 sec)\n",
      "INFO:tensorflow:global_step/sec: 90.0901\n",
      "INFO:tensorflow:loss = 1.6357081, step = 2501 (1.111 sec)\n",
      "INFO:tensorflow:global_step/sec: 93.7716\n",
      "INFO:tensorflow:loss = 3.4249454, step = 2601 (1.071 sec)\n",
      "INFO:tensorflow:global_step/sec: 108.654\n",
      "INFO:tensorflow:loss = 1.380976, step = 2701 (0.915 sec)\n",
      "INFO:tensorflow:global_step/sec: 134.267\n",
      "INFO:tensorflow:loss = 8.004151, step = 2801 (0.745 sec)\n",
      "INFO:tensorflow:global_step/sec: 142.636\n",
      "INFO:tensorflow:loss = 4.270151, step = 2901 (0.700 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 3000 into iris/models/iris_classifier/model.ckpt.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2018-08-13-13:46:50\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from iris/models/iris_classifier/model.ckpt-3000\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Finished evaluation at 2018-08-13-13:46:51\n",
      "INFO:tensorflow:Saving dict for global step 3000: accuracy = 0.9866667, average_loss = 0.03516959, global_step = 3000, loss = 1.7584796\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 3000: iris/models/iris_classifier/model.ckpt-3000\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Eval: None\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Classify: None\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Regress: None\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Predict: ['predict']\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Train: None\n",
      "INFO:tensorflow:Signatures EXCLUDED from export because they cannot be be served via TensorFlow Serving APIs:\n",
      "INFO:tensorflow:'serving_default' : Classification input must be a single string Tensor; got {'sepal_width': <tf.Tensor 'sepal_width:0' shape=(?,) dtype=float32>, 'petal_width': <tf.Tensor 'petal_width:0' shape=(?,) dtype=float32>, 'sepal_length': <tf.Tensor 'sepal_length:0' shape=(?,) dtype=float32>, 'petal_length': <tf.Tensor 'petal_length:0' shape=(?,) dtype=float32>}\n",
      "INFO:tensorflow:'classification' : Classification input must be a single string Tensor; got {'sepal_width': <tf.Tensor 'sepal_width:0' shape=(?,) dtype=float32>, 'petal_width': <tf.Tensor 'petal_width:0' shape=(?,) dtype=float32>, 'sepal_length': <tf.Tensor 'sepal_length:0' shape=(?,) dtype=float32>, 'petal_length': <tf.Tensor 'petal_length:0' shape=(?,) dtype=float32>}\n",
      "WARNING:tensorflow:Export includes no default signature!\n",
      "INFO:tensorflow:Restoring parameters from iris/models/iris_classifier/model.ckpt-3000\n",
      "INFO:tensorflow:Assets added to graph.\n",
      "INFO:tensorflow:No assets to write.\n",
      "INFO:tensorflow:SavedModel written to: iris/models/iris_classifier/export/estimate/temp-1534168011/saved_model.pb\n",
      "INFO:tensorflow:Loss for final step: 0.14819314.\n",
      ".......................................\n",
      "Experiment finished at 13:46:51\n",
      "\n",
      "Experiment elapsed time: 33.454834 seconds\n"
     ]
    }
   ],
   "source": [
    "run_experiment(hparams, run_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Inspect the Exported SavedModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iris/models/iris_classifier/export/estimate/1534168011\n",
      "saved_model.pb\n",
      "variables\n",
      "\n",
      "MetaGraphDef with tag-set: 'serve' contains the following SignatureDefs:\n",
      "\n",
      "signature_def['predict']:\n",
      "  The given SavedModel SignatureDef contains the following input(s):\n",
      "    inputs['petal_length'] tensor_info:\n",
      "        dtype: DT_FLOAT\n",
      "        shape: (-1)\n",
      "        name: petal_length:0\n",
      "    inputs['petal_width'] tensor_info:\n",
      "        dtype: DT_FLOAT\n",
      "        shape: (-1)\n",
      "        name: petal_width:0\n",
      "    inputs['sepal_length'] tensor_info:\n",
      "        dtype: DT_FLOAT\n",
      "        shape: (-1)\n",
      "        name: sepal_length:0\n",
      "    inputs['sepal_width'] tensor_info:\n",
      "        dtype: DT_FLOAT\n",
      "        shape: (-1)\n",
      "        name: sepal_width:0\n",
      "  The given SavedModel SignatureDef contains the following output(s):\n",
      "    outputs['class_ids'] tensor_info:\n",
      "        dtype: DT_INT64\n",
      "        shape: (-1, 1)\n",
      "        name: dnn/head/predictions/ExpandDims:0\n",
      "    outputs['classes'] tensor_info:\n",
      "        dtype: DT_STRING\n",
      "        shape: (-1, 1)\n",
      "        name: dnn/head/predictions/class_string_lookup_Lookup:0\n",
      "    outputs['logits'] tensor_info:\n",
      "        dtype: DT_FLOAT\n",
      "        shape: (-1, 3)\n",
      "        name: dnn/logits/BiasAdd:0\n",
      "    outputs['probabilities'] tensor_info:\n",
      "        dtype: DT_FLOAT\n",
      "        shape: (-1, 3)\n",
      "        name: dnn/head/predictions/probabilities:0\n",
      "  Method name is: tensorflow/serving/predict\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "saved_models_base=iris/models/iris_classifier/export/estimate/\n",
    "saved_model_dir=${saved_models_base}$(ls ${saved_models_base} | tail -n 1)\n",
    "echo ${saved_model_dir}\n",
    "ls ${saved_model_dir}\n",
    "saved_model_cli show --dir=${saved_model_dir} --all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction with SavedModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference_test(saved_model_dir, signature=\"predict\", size=100000):\n",
    "\n",
    "    tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "    \n",
    "    time_start = datetime.utcnow() \n",
    "    \n",
    "    predictor = tf.contrib.predictor.from_saved_model(\n",
    "        export_dir = saved_model_dir,\n",
    "        signature_def_key=signature\n",
    "    )\n",
    "    \n",
    "    output = predictor(\n",
    "        {\n",
    "            'sepal_length': range(size), \n",
    "            'sepal_width': range(size),  \n",
    "            'petal_length': range(size), \n",
    "            'petal_width': range(size)\n",
    "\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    \n",
    "    time_end = datetime.utcnow() \n",
    "\n",
    "    time_elapsed = time_end - time_start\n",
    "    print(\"Inference elapsed time: {} seconds\".format(time_elapsed.total_seconds()))\n",
    "    \n",
    "    print \"Sample Prediction output:\"\n",
    "    for key in output.keys():\n",
    "        print \"{}: {}\".format(key,output[key][0])\n",
    "\n",
    "    print(len(output['probabilities']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Test Prediction with SavedModel "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iris/models/iris_classifier/export/estimate/1534168011\n",
      "\n",
      "Inference elapsed time: 3.291696 seconds\n",
      "Sample Prediction output:\n",
      "probabilities: [4.1754058e-05 5.3128667e-07 9.9995768e-01]\n",
      "logits: [-1.0639172 -5.428168   9.019754 ]\n",
      "classes: ['versicolor']\n",
      "class_ids: [2]\n",
      "100000\n"
     ]
    }
   ],
   "source": [
    "export_dir = os.path.join(MODEL_DIR, \"export/estimate\")\n",
    "saved_model_dir = os.path.join(export_dir, os.listdir(export_dir)[-1]) \n",
    "print(saved_model_dir)\n",
    "print \"\"\n",
    "\n",
    "inference_test(saved_model_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Describe GraphDef"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def describe_graph(graph_def, show_nodes=False):\n",
    "    \n",
    "    print 'Input Features: {}'.format([node.name for node in graph_def.node if node.op=='Placeholder'])\n",
    "    print 'Output Probabilities: {}'.format( [node.name for node in graph_def.node if node.op=='Softmax'])\n",
    "    print 'Constant Count: {}'.format( len([node for node in graph_def.node if node.op=='Const']))\n",
    "    print 'Variable Count: {}'.format( len([node for node in graph_def.node if 'Variable' in node.op]))\n",
    "    print 'Identity Count: {}'.format( len([node for node in graph_def.node if node.op=='Identity']))\n",
    "    print 'Total nodes: {}'.format( len(graph_def.node))\n",
    "    print ''\n",
    "    \n",
    "    if show_nodes==True:\n",
    "        for node in graph_def.node:\n",
    "            print(node.op, node.name)\n",
    "           #print(node.op, node.name, node.attr['value'].tensor)\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Describe the SavedModel Graph (before optimisation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load GraphDef from a SavedModel Directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_graph_def_from_saved_model(saved_model_dir):\n",
    "    \n",
    "    print saved_model_dir\n",
    "    print \"\"\n",
    "    \n",
    "    from tensorflow.python.saved_model import tag_constants\n",
    "    \n",
    "    with tf.Session() as session:\n",
    "        meta_graph_def = tf.saved_model.loader.load(\n",
    "            session,\n",
    "            tags=[tag_constants.SERVING],\n",
    "            export_dir=saved_model_dir\n",
    "        )\n",
    "        \n",
    "    return meta_graph_def.graph_def"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iris/models/iris_classifier/export/estimate/1534168011\n",
      "\n",
      "Input Features: [u'sepal_width', u'petal_width', u'sepal_length', u'petal_length']\n",
      "Output Probabilities: [u'dnn/head/predictions/probabilities']\n",
      "Constant Count: 84\n",
      "Variable Count: 11\n",
      "Identity Count: 23\n",
      "Total nodes: 252\n",
      "\n"
     ]
    }
   ],
   "source": [
    "describe_graph(get_graph_def_from_saved_model(saved_model_dir))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get model size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_size(model_dir):\n",
    "    \n",
    "    print model_dir\n",
    "    print \"\"\n",
    "    \n",
    "    pb_size = os.path.getsize(os.path.join(model_dir,'saved_model.pb'))\n",
    "    \n",
    "    variables_size = 0\n",
    "    if os.path.exists(os.path.join(model_dir,'variables/variables.data-00000-of-00001')):\n",
    "        variables_size = os.path.getsize(os.path.join(model_dir,'variables/variables.data-00000-of-00001'))\n",
    "        variables_size += os.path.getsize(os.path.join(model_dir,'variables/variables.index'))\n",
    "\n",
    "    print \"Model siz: {} KB\".format(round(pb_size/(1024.0),3))\n",
    "    print \"Variables size: {} KB\".format(round( variables_size/(1024.0),3))\n",
    "    print \"Total Size: {} KB\".format(round((pb_size + variables_size)/(1024.0),3))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iris/models/iris_classifier/export/estimate/1534168011\n",
      "\n",
      "Model siz: 51.087 KB\n",
      "Variables size: 3094.483 KB\n",
      "Total Size: 3145.57 KB\n"
     ]
    }
   ],
   "source": [
    "get_size(saved_model_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Freeze SavedModel\n",
    "\n",
    "This function will convert the SavedModel into a GraphDef file (freezed_model.pb), and storing the variables as constrant to the freezed_model.pb\n",
    "\n",
    "You need to define the graph output nodes for freezing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def freeze_graph(saved_model_dir):\n",
    "    \n",
    "    from tensorflow.python.tools import freeze_graph\n",
    "    from tensorflow.python.saved_model import tag_constants\n",
    "    \n",
    "    output_graph_filename = os.path.join(saved_model_dir, \"freezed_model.pb\")\n",
    "    output_node_names = \"dnn/head/predictions/probabilities\"\n",
    "    #output_node_names += \", dnn/head/predictions/ExpandDims, dnn/head/predictions/class_string_lookup_Lookup\"\n",
    "    initializer_nodes = \"\"\n",
    "\n",
    "    freeze_graph.freeze_graph(\n",
    "        input_saved_model_dir=saved_model_dir,\n",
    "        output_graph=output_graph_filename,\n",
    "        saved_model_tags = tag_constants.SERVING,\n",
    "        output_node_names=output_node_names,\n",
    "        initializer_nodes=initializer_nodes,\n",
    "\n",
    "        input_graph=None, \n",
    "        input_saver=False,\n",
    "        input_binary=False, \n",
    "        input_checkpoint=None, \n",
    "        restore_op_name=None, \n",
    "        filename_tensor_name=None, \n",
    "        clear_devices=False,\n",
    "        input_meta_graph=False,\n",
    "    )\n",
    "    \n",
    "    print \"SavedModel graph freezed!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SavedModel graph freezed!\n"
     ]
    }
   ],
   "source": [
    "freeze_graph(saved_model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iris/models/iris_classifier/export/estimate/1534168011\n",
      "freezed_model.pb\n",
      "saved_model.pb\n",
      "variables\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "saved_models_base=iris/models/iris_classifier/export/estimate/\n",
    "saved_model_dir=${saved_models_base}$(ls ${saved_models_base} | tail -n 1)\n",
    "echo ${saved_model_dir}\n",
    "ls ${saved_model_dir}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Describe the freezed_model.pb Graph (after freezing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load GraphDef from GraphDef File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_graph_def_from_file(graph_filepath):\n",
    "    \n",
    "    print graph_filepath\n",
    "    print \"\"\n",
    "    \n",
    "    from tensorflow.python import ops\n",
    "    \n",
    "    with ops.Graph().as_default():\n",
    "        with tf.gfile.GFile(graph_filepath, \"rb\") as f:\n",
    "            graph_def = tf.GraphDef()\n",
    "            graph_def.ParseFromString(f.read())\n",
    "            \n",
    "            return graph_def\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iris/models/iris_classifier/export/estimate/1534168011/freezed_model.pb\n",
      "\n",
      "Input Features: [u'sepal_width', u'petal_width', u'sepal_length', u'petal_length']\n",
      "Output Probabilities: [u'dnn/head/predictions/probabilities']\n",
      "Constant Count: 31\n",
      "Variable Count: 0\n",
      "Identity Count: 20\n",
      "Total nodes: 91\n",
      "\n"
     ]
    }
   ],
   "source": [
    "freezed_filepath=os.path.join(saved_model_dir,'freezed_model.pb')\n",
    "describe_graph(get_graph_def_from_file(freezed_filepath))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Optimise the freezed_model.pb\n",
    "\n",
    "Note that, the optimised graph will replace freezed_model.pb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimise GraphDef"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_graph(model_dir, graph_filename, transforms):\n",
    "    \n",
    "    from tensorflow.tools.graph_transforms import TransformGraph\n",
    "    \n",
    "    input_names = []\n",
    "    output_names = ['dnn/head/predictions/probabilities']\n",
    "    \n",
    "    graph_def = get_graph_def_from_file(os.path.join(model_dir, graph_filename))\n",
    "    optimised_graph_def = TransformGraph(graph_def, \n",
    "                                         input_names,\n",
    "                                         output_names,\n",
    "                                         transforms \n",
    "                                        )\n",
    "    tf.train.write_graph(optimised_graph_def,\n",
    "                        logdir=model_dir,\n",
    "                        as_text=False,\n",
    "                        name='optimised_model.pb')\n",
    "    \n",
    "    print \"Freezed graph optimised!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iris/models/iris_classifier/export/estimate/1534168011/freezed_model.pb\n",
      "\n",
      "Freezed graph optimised!\n"
     ]
    }
   ],
   "source": [
    "transforms = [\n",
    "    'remove_nodes(op=Identity)', \n",
    "    'fold_constants(ignore_errors=true)',\n",
    "    'fold_batch_norms',\n",
    "    'fold_old_batch_norms',\n",
    "    'round_weights(num_steps=256)',\n",
    "    'quantize_weights', \n",
    "#   'quantize_nodes',\n",
    "#   'merge_duplicate_nodes',\n",
    "    'strip_unused_nodes', \n",
    "    'sort_by_execution_order'\n",
    "]\n",
    "\n",
    "optimize_graph(saved_model_dir, 'freezed_model.pb', transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iris/models/iris_classifier/export/estimate/1534168011\n",
      "freezed_model.pb\n",
      "optimised_model.pb\n",
      "saved_model.pb\n",
      "variables\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "saved_models_base=iris/models/iris_classifier/export/estimate/\n",
    "saved_model_dir=${saved_models_base}$(ls ${saved_models_base} | tail -n 1)\n",
    "echo ${saved_model_dir}\n",
    "ls ${saved_model_dir}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Describe the Optimised Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iris/models/iris_classifier/export/estimate/1534168011/optimised_model.pb\n",
      "\n",
      "Input Features: [u'petal_length', u'sepal_length', u'petal_width', u'sepal_width']\n",
      "Output Probabilities: [u'dnn/head/predictions/probabilities']\n",
      "Constant Count: 41\n",
      "Variable Count: 0\n",
      "Identity Count: 0\n",
      "Total nodes: 86\n",
      "\n"
     ]
    }
   ],
   "source": [
    "optimised_filepath=os.path.join(saved_model_dir,'optimised_model.pb')\n",
    "describe_graph(get_graph_def_from_file(optimised_filepath))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Convert (optimised) graph (GraphDef) to SavedModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_graph_def_to_saved_model(graph_filepath):\n",
    "\n",
    "    from tensorflow.python import ops\n",
    "    export_dir=os.path.join(saved_model_dir,'optimised')\n",
    "\n",
    "    if tf.gfile.Exists(export_dir):\n",
    "        tf.gfile.DeleteRecursively(export_dir)\n",
    "\n",
    "    graph_def = get_graph_def_from_file(graph_filepath)\n",
    "    \n",
    "    with tf.Session(graph=tf.Graph()) as session:\n",
    "        tf.import_graph_def(graph_def, name=\"\")\n",
    "        tf.saved_model.simple_save(session,\n",
    "                export_dir,\n",
    "                inputs={\n",
    "                    node.name: session.graph.get_tensor_by_name(\"{}:0\".format(node.name)) \n",
    "                    for node in graph_def.node if node.op=='Placeholder'},\n",
    "                outputs={\n",
    "                    \"probabilities\": session.graph.get_tensor_by_name(\"dnn/head/predictions/probabilities:0\"),\n",
    "                    #\"class_ids\": g.get_tensor_by_name(\"dnn/head/predictions/ExpandDims:0\"),\n",
    "                    #\"classes\": g.get_tensor_by_name(\"dnn/head/predictions/class_string_lookup_Lookup:0\"),\n",
    "                }\n",
    "            )\n",
    "\n",
    "        print \"Optimised graph converted to SavedModel!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iris/models/iris_classifier/export/estimate/1534168011/optimised_model.pb\n",
      "\n",
      "Optimised graph converted to SavedModel!\n"
     ]
    }
   ],
   "source": [
    "optimised_filepath=os.path.join(saved_model_dir,'optimised_model.pb')\n",
    "convert_graph_def_to_saved_model(optimised_filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimised SavedModel Size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iris/models/iris_classifier/export/estimate/1534168011/optimised\n",
      "\n",
      "Model siz: 796.803 KB\n",
      "Variables size: 0.0 KB\n",
      "Total Size: 796.803 KB\n"
     ]
    }
   ],
   "source": [
    "optimised_saved_model_dir = os.path.join(saved_model_dir,'optimised') \n",
    "get_size(optimised_saved_model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved_model.pb\n",
      "variables\n",
      "\n",
      "MetaGraphDef with tag-set: 'serve' contains the following SignatureDefs:\n",
      "\n",
      "signature_def['serving_default']:\n",
      "  The given SavedModel SignatureDef contains the following input(s):\n",
      "    inputs['petal_length'] tensor_info:\n",
      "        dtype: DT_FLOAT\n",
      "        shape: (-1)\n",
      "        name: petal_length:0\n",
      "    inputs['petal_width'] tensor_info:\n",
      "        dtype: DT_FLOAT\n",
      "        shape: (-1)\n",
      "        name: petal_width:0\n",
      "    inputs['sepal_length'] tensor_info:\n",
      "        dtype: DT_FLOAT\n",
      "        shape: (-1)\n",
      "        name: sepal_length:0\n",
      "    inputs['sepal_width'] tensor_info:\n",
      "        dtype: DT_FLOAT\n",
      "        shape: (-1)\n",
      "        name: sepal_width:0\n",
      "  The given SavedModel SignatureDef contains the following output(s):\n",
      "    outputs['probabilities'] tensor_info:\n",
      "        dtype: DT_FLOAT\n",
      "        shape: (-1, 3)\n",
      "        name: dnn/head/predictions/probabilities:0\n",
      "  Method name is: tensorflow/serving/predict\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "saved_models_base=iris/models/iris_classifier/export/estimate/\n",
    "saved_model_dir=${saved_models_base}$(ls ${saved_models_base} | tail -n 1)/optimised\n",
    "ls ${saved_model_dir}\n",
    "saved_model_cli show --dir ${saved_model_dir} --all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Prediction with the Optimised SavedModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iris/models/iris_classifier/export/estimate/1534168011/optimised\n",
      "Inference elapsed time: 2.710049 seconds\n",
      "Sample Prediction output:\n",
      "probabilities: [4.0991668e-05 4.2484294e-07 9.9995863e-01]\n",
      "100000\n"
     ]
    }
   ],
   "source": [
    "freezed_saved_model_dir = os.path.join(saved_model_dir,'optimised') \n",
    "print(freezed_saved_model_dir)\n",
    "inference_test(saved_model_dir=freezed_saved_model_dir, signature='serving_default')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
