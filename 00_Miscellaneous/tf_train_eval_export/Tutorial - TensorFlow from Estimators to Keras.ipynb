{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TensorFlow: From Estimators to Keras\n",
    "* Building a custom TensorFlow estimator (as a reference)\n",
    "    1. Use **Census** classification dataset\n",
    "    2. Create **feature columns** from the estimator\n",
    "    3. Implement a **tf.data input_fn**\n",
    "    4. Create a custom estimator using **tf.keras.layers**\n",
    "    5. **Train** and **evaluate** the model\n",
    "* Building a Functional Keras model and using tf.data APIs\n",
    "    1. Modify the **input_fn** to process categorical features\n",
    "    2. Build a Functional Keras Model\n",
    "    3. Use the input_fn to fit the Keras model\n",
    "    4. Configure **epochs** and **validation**\n",
    "    5. Configure **callbacks** for **early stopping** and **checkpoints**\n",
    "* Save and Load Keras model\n",
    "* Export Keras model to saved_model\n",
    "* Converting Keras model to estimator\n",
    "* Concluding Remarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow : 1.12.0\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import data\n",
    "\n",
    "print \"TensorFlow : {}\".format(tf.__version__)\n",
    "\n",
    "SEED = 19831060"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR='data'\n",
    "# !mkdir $DATA_DIR\n",
    "# !gsutil cp gs://cloud-samples-data/ml-engine/census/data/adult.data.csv $DATA_DIR\n",
    "# !gsutil cp gs://cloud-samples-data/ml-engine/census/data/adult.test.csv $DATA_DIR\n",
    "TRAIN_DATA_FILE = os.path.join(DATA_DIR, 'adult.data.csv')\n",
    "EVAL_DATA_FILE = os.path.join(DATA_DIR, 'adult.test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_DATA_SIZE = 32561\n",
    "EVAL_DATA_SIZE = 16278"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "HEADER = ['age', 'workclass', 'fnlwgt', 'education', 'education_num',\n",
    "               'marital_status', 'occupation', 'relationship', 'race', 'gender',\n",
    "               'capital_gain', 'capital_loss', 'hours_per_week',\n",
    "               'native_country', 'income_bracket']\n",
    "\n",
    "HEADER_DEFAULTS = [[0], [''], [0], [''], [0], [''], [''], [''], [''], [''],\n",
    "                       [0], [0], [0], [''], ['']]\n",
    "\n",
    "NUMERIC_FEATURE_NAMES = ['age', 'education_num', 'capital_gain', 'capital_loss', 'hours_per_week']\n",
    "CATEGORICAL_FEATURE_NAMES = ['gender', 'race', 'education', 'marital_status', 'relationship', \n",
    "                             'workclass', 'occupation', 'native_country']\n",
    "\n",
    "FEATURE_NAMES = NUMERIC_FEATURE_NAMES + CATEGORICAL_FEATURE_NAMES\n",
    "TARGET_NAME = 'income_bracket'\n",
    "TARGET_LABELS = [' <=50K', ' >50K']\n",
    "WEIGHT_COLUMN_NAME = 'fnlwgt'\n",
    "NUM_CLASSES = len(TARGET_LABELS)\n",
    "\n",
    "def get_categorical_features_vocabolary():\n",
    "    data = pd.read_csv(TRAIN_DATA_FILE, names=HEADER)\n",
    "    return {\n",
    "        column: list(data[column].unique()) \n",
    "        for column in data.columns if column in CATEGORICAL_FEATURE_NAMES\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'workclass': [' State-gov', ' Self-emp-not-inc', ' Private', ' Federal-gov', ' Local-gov', ' ?', ' Self-emp-inc', ' Without-pay', ' Never-worked'], 'relationship': [' Not-in-family', ' Husband', ' Wife', ' Own-child', ' Unmarried', ' Other-relative'], 'gender': [' Male', ' Female'], 'marital_status': [' Never-married', ' Married-civ-spouse', ' Divorced', ' Married-spouse-absent', ' Separated', ' Married-AF-spouse', ' Widowed'], 'race': [' White', ' Black', ' Asian-Pac-Islander', ' Amer-Indian-Eskimo', ' Other'], 'native_country': [' United-States', ' Cuba', ' Jamaica', ' India', ' ?', ' Mexico', ' South', ' Puerto-Rico', ' Honduras', ' England', ' Canada', ' Germany', ' Iran', ' Philippines', ' Italy', ' Poland', ' Columbia', ' Cambodia', ' Thailand', ' Ecuador', ' Laos', ' Taiwan', ' Haiti', ' Portugal', ' Dominican-Republic', ' El-Salvador', ' France', ' Guatemala', ' China', ' Japan', ' Yugoslavia', ' Peru', ' Outlying-US(Guam-USVI-etc)', ' Scotland', ' Trinadad&Tobago', ' Greece', ' Nicaragua', ' Vietnam', ' Hong', ' Ireland', ' Hungary', ' Holand-Netherlands'], 'education': [' Bachelors', ' HS-grad', ' 11th', ' Masters', ' 9th', ' Some-college', ' Assoc-acdm', ' Assoc-voc', ' 7th-8th', ' Doctorate', ' Prof-school', ' 5th-6th', ' 10th', ' 1st-4th', ' Preschool', ' 12th'], 'occupation': [' Adm-clerical', ' Exec-managerial', ' Handlers-cleaners', ' Prof-specialty', ' Other-service', ' Sales', ' Craft-repair', ' Transport-moving', ' Farming-fishing', ' Machine-op-inspct', ' Tech-support', ' ?', ' Protective-serv', ' Armed-Forces', ' Priv-house-serv']}\n"
     ]
    }
   ],
   "source": [
    "feature_vocabolary = get_categorical_features_vocabolary()\n",
    "print(feature_vocabolary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a TensorFlow Custom Estimator\n",
    "\n",
    "1. Creating feature columns\n",
    "2. Creating model_fn\n",
    "3. Create estimator using the model_fn\n",
    "4. Define data input_fn\n",
    "5. Define Train and evaluate experiment\n",
    "6. Run experiment with parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Create feature columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_feature_columns():\n",
    "    \n",
    "    feature_columns = []\n",
    "    \n",
    "    for column in NUMERIC_FEATURE_NAMES:\n",
    "        feature_column = tf.feature_column.numeric_column(column)\n",
    "        feature_columns.append(feature_column)\n",
    "        \n",
    "    for column in CATEGORICAL_FEATURE_NAMES:\n",
    "        vocabolary = feature_vocabolary[column]\n",
    "        embed_size = int(math.sqrt(len(vocabolary)))\n",
    "        feature_column = tf.feature_column.embedding_column(\n",
    "            tf.feature_column.categorical_column_with_vocabulary_list(column, vocabolary), \n",
    "            embed_size)\n",
    "        feature_columns.append(feature_column)\n",
    "        \n",
    "    return feature_columns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Create model_fn\n",
    "1. Use feature columns to create input_layer\n",
    "2. Use tf.keras.layers to define the model architecutre and output\n",
    "3. Use binary_classification_head for create EstimatorSpec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_fn(features, labels, mode, params):\n",
    "    \n",
    "    is_training = True if mode == tf.estimator.ModeKeys.TRAIN else False\n",
    "    \n",
    "    # model body\n",
    "    def _inference(features, mode, params):\n",
    "        \n",
    "        feature_columns = create_feature_columns()\n",
    "        input_layer = tf.feature_column.input_layer(features=features, feature_columns=feature_columns)\n",
    "        dense_inputs = input_layer\n",
    "        for i in range(len(params.hidden_units)):\n",
    "            dense = tf.keras.layers.Dense(params.hidden_units[i], activation='relu')(dense_inputs)\n",
    "            dense_dropout = tf.keras.layers.Dropout(params.dropout_prob)(dense, training=is_training)\n",
    "            dense_inputs = dense_dropout\n",
    "        fully_connected = dense_inputs  \n",
    "        logits = tf.keras.layers.Dense(units=1, name='logits', activation=None)(fully_connected)\n",
    "        return logits\n",
    "    \n",
    "    # model head\n",
    "    head = tf.contrib.estimator.binary_classification_head(\n",
    "        label_vocabulary=TARGET_LABELS,\n",
    "        weight_column=WEIGHT_COLUMN_NAME\n",
    "    )\n",
    "    \n",
    "    return head.create_estimator_spec(\n",
    "        features=features,\n",
    "        mode=mode,\n",
    "        logits=_inference(features, mode, params),\n",
    "        labels=labels,\n",
    "        optimizer=tf.train.AdamOptimizer(params.learning_rate)\n",
    "    )\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Create estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_estimator(params, run_config):\n",
    "    \n",
    "    feature_columns = create_feature_columns()\n",
    "    \n",
    "    estimator = tf.estimator.Estimator(\n",
    "        model_fn,\n",
    "        params=params,\n",
    "        config=run_config\n",
    "    )\n",
    "    \n",
    "    return estimator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Data Input Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_input_fn(file_pattern, batch_size, num_epochs, \n",
    "                  mode=tf.estimator.ModeKeys.EVAL):\n",
    "    \n",
    "    def _input_fn():\n",
    "        dataset = tf.data.experimental.make_csv_dataset(\n",
    "            file_pattern=file_pattern,\n",
    "            batch_size=batch_size,\n",
    "            column_names=HEADER,\n",
    "            column_defaults=HEADER_DEFAULTS,\n",
    "            label_name=TARGET_NAME,\n",
    "            field_delim=',',\n",
    "            use_quote_delim=True,\n",
    "            header=False,\n",
    "            num_epochs=num_epochs,\n",
    "            shuffle= (mode==tf.estimator.ModeKeys.TRAIN)\n",
    "        )\n",
    "        \n",
    "        iterator = dataset.make_one_shot_iterator()\n",
    "        features, target = iterator.get_next()\n",
    "        return features, target\n",
    "    \n",
    "    return _input_fn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Experiment Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate_experiment(params, run_config):\n",
    "    \n",
    "    # TrainSpec ####################################\n",
    "    train_input_fn = make_input_fn(\n",
    "        TRAIN_DATA_FILE,\n",
    "        batch_size=params.batch_size,\n",
    "        num_epochs=None,\n",
    "        mode=tf.estimator.ModeKeys.TRAIN\n",
    "    )\n",
    "    \n",
    "    train_spec = tf.estimator.TrainSpec(\n",
    "        input_fn = train_input_fn,\n",
    "        max_steps=params.traning_steps\n",
    "    )\n",
    "    ###############################################    \n",
    "    \n",
    "    # EvalSpec ####################################\n",
    "    eval_input_fn = make_input_fn(\n",
    "        EVAL_DATA_FILE,\n",
    "        num_epochs=1,\n",
    "        batch_size=params.batch_size,\n",
    "    )\n",
    "\n",
    "    eval_spec = tf.estimator.EvalSpec(\n",
    "        name=datetime.utcnow().strftime(\"%H%M%S\"),\n",
    "        input_fn = eval_input_fn,\n",
    "        steps=None,\n",
    "        start_delay_secs=0,\n",
    "        throttle_secs=params.eval_throttle_secs\n",
    "    )\n",
    "    ###############################################\n",
    "\n",
    "    tf.logging.set_verbosity(tf.logging.INFO)\n",
    "    \n",
    "    if tf.gfile.Exists(run_config.model_dir):\n",
    "        print(\"Removing previous artefacts...\")\n",
    "        tf.gfile.DeleteRecursively(run_config.model_dir)\n",
    "            \n",
    "    print ''\n",
    "    estimator = create_estimator(params, run_config)\n",
    "    print ''\n",
    "    \n",
    "    time_start = datetime.utcnow() \n",
    "    print(\"Experiment started at {}\".format(time_start.strftime(\"%H:%M:%S\")))\n",
    "    print(\".......................................\") \n",
    "\n",
    "    tf.estimator.train_and_evaluate(\n",
    "        estimator=estimator,\n",
    "        train_spec=train_spec, \n",
    "        eval_spec=eval_spec\n",
    "    )\n",
    "\n",
    "    time_end = datetime.utcnow() \n",
    "    print(\".......................................\")\n",
    "    print(\"Experiment finished at {}\".format(time_end.strftime(\"%H:%M:%S\")))\n",
    "    print(\"\")\n",
    "    time_elapsed = time_end - time_start\n",
    "    print(\"Experiment elapsed time: {} seconds\".format(time_elapsed.total_seconds()))\n",
    "    \n",
    "    return estimator\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Run Experiment with Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "INFO:tensorflow:Using config: {'_save_checkpoints_secs': None, '_session_config': allow_soft_placement: true\n",
      "graph_options {\n",
      "  rewrite_options {\n",
      "    meta_optimizer_iterations: ONE\n",
      "  }\n",
      "}\n",
      ", '_keep_checkpoint_max': 3, '_task_type': 'worker', '_train_distribute': None, '_is_chief': True, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x121253e10>, '_model_dir': 'models/census/dnn_classifier', '_protocol': None, '_save_checkpoints_steps': 200, '_keep_checkpoint_every_n_hours': 10000, '_service': None, '_num_ps_replicas': 0, '_tf_random_seed': 19831060, '_save_summary_steps': 100, '_device_fn': None, '_experimental_distribute': None, '_num_worker_replicas': 1, '_task_id': 0, '_log_step_count_steps': 100, '_evaluation_master': '', '_eval_distribute': None, '_global_id_in_cluster': 0, '_master': ''}\n",
      "\n",
      "Experiment started at 17:51:39\n",
      ".......................................\n",
      "INFO:tensorflow:Not using Distribute Coordinator.\n",
      "INFO:tensorflow:Running training and evaluation locally (non-distributed).\n",
      "INFO:tensorflow:Start train and evaluate loop. The evaluate will happen after every checkpoint. Checkpoint frequency is determined based on RunConfig arguments: save_checkpoints_steps 200 or save_checkpoints_secs None.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 0 into models/census/dnn_classifier/model.ckpt.\n",
      "INFO:tensorflow:loss = 9163679.0, step = 1\n",
      "INFO:tensorflow:global_step/sec: 83.2043\n",
      "INFO:tensorflow:loss = 91374.14, step = 101 (1.203 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 200 into models/census/dnn_classifier/model.ckpt.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "WARNING:tensorflow:Trapezoidal rule is known to produce incorrect PR-AUCs; please switch to \"careful_interpolation\" instead.\n",
      "WARNING:tensorflow:Trapezoidal rule is known to produce incorrect PR-AUCs; please switch to \"careful_interpolation\" instead.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2019-01-18-17:51:49\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from models/census/dnn_classifier/model.ckpt-200\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Finished evaluation at 2019-01-18-17:51:51\n",
      "INFO:tensorflow:Saving dict for global step 200: accuracy = 0.80221015, accuracy_baseline = 0.7638036, auc = 0.86573344, auc_precision_recall = 0.61520016, average_loss = 0.37370154, global_step = 200, label/mean = 0.23619643, loss = 70839.79, precision = 0.6660012, prediction/mean = 0.24851921, recall = 0.3261855\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 200: models/census/dnn_classifier/model.ckpt-200\n",
      "INFO:tensorflow:global_step/sec: 18.1498\n",
      "INFO:tensorflow:loss = 67082.9, step = 201 (5.510 sec)\n",
      "INFO:tensorflow:global_step/sec: 102.239\n",
      "INFO:tensorflow:loss = 66657.33, step = 301 (0.979 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 400 into models/census/dnn_classifier/model.ckpt.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "WARNING:tensorflow:Trapezoidal rule is known to produce incorrect PR-AUCs; please switch to \"careful_interpolation\" instead.\n",
      "WARNING:tensorflow:Trapezoidal rule is known to produce incorrect PR-AUCs; please switch to \"careful_interpolation\" instead.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2019-01-18-17:51:58\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from models/census/dnn_classifier/model.ckpt-400\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Finished evaluation at 2019-01-18-17:51:59\n",
      "INFO:tensorflow:Saving dict for global step 400: accuracy = 0.81460094, accuracy_baseline = 0.7638036, auc = 0.87952614, auc_precision_recall = 0.6394008, average_loss = 0.3608392, global_step = 400, label/mean = 0.23619643, loss = 68388.65, precision = 0.5827283, prediction/mean = 0.2720198, recall = 0.75744414\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 400: models/census/dnn_classifier/model.ckpt-400\n",
      "INFO:tensorflow:global_step/sec: 14.6577\n",
      "INFO:tensorflow:loss = 75905.75, step = 401 (6.822 sec)\n",
      "INFO:tensorflow:global_step/sec: 184.209\n",
      "INFO:tensorflow:loss = 67112.35, step = 501 (0.543 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 600 into models/census/dnn_classifier/model.ckpt.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "WARNING:tensorflow:Trapezoidal rule is known to produce incorrect PR-AUCs; please switch to \"careful_interpolation\" instead.\n",
      "WARNING:tensorflow:Trapezoidal rule is known to produce incorrect PR-AUCs; please switch to \"careful_interpolation\" instead.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2019-01-18-17:52:02\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from models/census/dnn_classifier/model.ckpt-600\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Finished evaluation at 2019-01-18-17:52:04\n",
      "INFO:tensorflow:Saving dict for global step 600: accuracy = 0.822667, accuracy_baseline = 0.7638036, auc = 0.882122, auc_precision_recall = 0.650107, average_loss = 0.3522455, global_step = 600, label/mean = 0.23619643, loss = 66743.04, precision = 0.6086635, prediction/mean = 0.2400031, recall = 0.69796675\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 600: models/census/dnn_classifier/model.ckpt-600\n",
      "INFO:tensorflow:global_step/sec: 20.7612\n",
      "INFO:tensorflow:loss = 69297.555, step = 601 (4.816 sec)\n",
      "INFO:tensorflow:global_step/sec: 139.734\n",
      "INFO:tensorflow:loss = 80328.125, step = 701 (0.718 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 800 into models/census/dnn_classifier/model.ckpt.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "WARNING:tensorflow:Trapezoidal rule is known to produce incorrect PR-AUCs; please switch to \"careful_interpolation\" instead.\n",
      "WARNING:tensorflow:Trapezoidal rule is known to produce incorrect PR-AUCs; please switch to \"careful_interpolation\" instead.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2019-01-18-17:52:08\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from models/census/dnn_classifier/model.ckpt-800\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Finished evaluation at 2019-01-18-17:52:09\n",
      "INFO:tensorflow:Saving dict for global step 800: accuracy = 0.82035464, accuracy_baseline = 0.7638036, auc = 0.88284963, auc_precision_recall = 0.64829445, average_loss = 0.3525522, global_step = 800, label/mean = 0.23619643, loss = 66794.64, precision = 0.5998432, prediction/mean = 0.2543438, recall = 0.7192128\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 800: models/census/dnn_classifier/model.ckpt-800\n",
      "INFO:tensorflow:global_step/sec: 23.4158\n",
      "INFO:tensorflow:loss = 51990.57, step = 801 (4.272 sec)\n",
      "INFO:tensorflow:global_step/sec: 172.192\n",
      "INFO:tensorflow:loss = 78253.14, step = 901 (0.578 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 1000 into models/census/dnn_classifier/model.ckpt.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "WARNING:tensorflow:Trapezoidal rule is known to produce incorrect PR-AUCs; please switch to \"careful_interpolation\" instead.\n",
      "WARNING:tensorflow:Trapezoidal rule is known to produce incorrect PR-AUCs; please switch to \"careful_interpolation\" instead.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2019-01-18-17:52:13\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from models/census/dnn_classifier/model.ckpt-1000\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Finished evaluation at 2019-01-18-17:52:15\n",
      "INFO:tensorflow:Saving dict for global step 1000: accuracy = 0.82400525, accuracy_baseline = 0.7638036, auc = 0.883553, auc_precision_recall = 0.65175796, average_loss = 0.3573896, global_step = 1000, label/mean = 0.23619643, loss = 67732.695, precision = 0.62925595, prediction/mean = 0.24527113, recall = 0.62041414\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 1000: models/census/dnn_classifier/model.ckpt-1000\n",
      "INFO:tensorflow:Loss for final step: 73616.766.\n",
      ".......................................\n",
      "Experiment finished at 17:52:15\n",
      "\n",
      "Experiment elapsed time: 35.751717 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.estimator.estimator.Estimator at 0x121e7ff50>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MODELS_LOCATION = 'models/census'\n",
    "MODEL_NAME = 'dnn_classifier'\n",
    "model_dir = os.path.join(MODELS_LOCATION, MODEL_NAME)\n",
    "\n",
    "params  = tf.contrib.training.HParams(\n",
    "    batch_size=200,\n",
    "    traning_steps=1000,\n",
    "    hidden_units=[100, 70, 50],\n",
    "    learning_rate=0.01,\n",
    "    dropout_prob=0.2,\n",
    "    eval_throttle_secs=0,\n",
    ")\n",
    "\n",
    "strategy = tf.contrib.distribute.MirroredStrategy(num_gpus=4)\n",
    "\n",
    "run_config = tf.estimator.RunConfig(\n",
    "    tf_random_seed=SEED,\n",
    "    save_checkpoints_steps=200,\n",
    "    keep_checkpoint_max=3,\n",
    "    model_dir=model_dir,\n",
    "    #train_distribute=strategy # use for multiple GPUs training\n",
    ")\n",
    "\n",
    "train_and_evaluate_experiment(params, run_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a Keras Model\n",
    "1. Implement a data input_fn process the data for the Keras model\n",
    "2. Create the Keras model\n",
    "3. Create the callbacks\n",
    "4. Run the experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Data input_fn\n",
    "1. Create lookups for categorical features vocabolary to numerical index\n",
    "2. Process the dataset features to:\n",
    "    * extrat the instance weight column\n",
    "    * convert the categorical features to numerical index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_keras_input_fn(file_pattern, batch_size, mode=tf.estimator.ModeKeys.EVAL):\n",
    "    \n",
    "    mapping_tables = {}\n",
    "        \n",
    "    mapping_tables[TARGET_NAME] = tf.contrib.lookup.index_table_from_tensor(\n",
    "        mapping=tf.constant(TARGET_LABELS))\n",
    "\n",
    "    for feature_name in CATEGORICAL_FEATURE_NAMES:\n",
    "        mapping_tables[feature_name] = tf.contrib.lookup.index_table_from_tensor(\n",
    "            mapping=tf.constant(feature_vocabolary[feature_name]))\n",
    "    try:\n",
    "        tf.tables_initializer().run(session=tf.keras.backend.get_session()) \n",
    "    except:\n",
    "        pass\n",
    "            \n",
    "    def _process_features(features, target):\n",
    "        \n",
    "        weight = features.pop(WEIGHT_COLUMN_NAME)\n",
    "        target = mapping_tables[TARGET_NAME].lookup(target)\n",
    "        for feature in CATEGORICAL_FEATURE_NAMES:\n",
    "            features[feature] = mapping_tables[feature].lookup(features[feature])\n",
    "        return features, target, weight\n",
    "                        \n",
    "    def _input_fn():\n",
    "        \n",
    "        dataset = tf.data.experimental.make_csv_dataset(\n",
    "            file_pattern=file_pattern,\n",
    "            batch_size=batch_size,\n",
    "            column_names=HEADER,\n",
    "            column_defaults=HEADER_DEFAULTS,\n",
    "            label_name=TARGET_NAME,\n",
    "            field_delim=',',\n",
    "            use_quote_delim=True,\n",
    "            header=False,\n",
    "            shuffle= (mode==tf.estimator.ModeKeys.TRAIN)\n",
    "        ).map(_process_features)\n",
    "\n",
    "        return dataset\n",
    "    \n",
    "    return _input_fn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Create the keras model\n",
    "1. Create the model architecture:\n",
    "    * One input for each feature\n",
    "    * Embedding layer for each categorical feature\n",
    "    * Sigmoid output\n",
    "2. Compile the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(params):\n",
    "    \n",
    "    inputs = []\n",
    "    to_concat = []\n",
    "    \n",
    "#     mapping_tables = {}\n",
    "#     for feature_name in CATEGORICAL_FEATURE_NAMES:\n",
    "#         mapping_tables[feature_name] = tf.contrib.lookup.index_table_from_tensor(\n",
    "#             mapping=tf.constant(feature_vocabolary[feature_name]))\n",
    "#     try:\n",
    "#         tf.tables_initializer().run(session=tf.keras.backend.get_session()) \n",
    "#     except:\n",
    "#         pass\n",
    "    \n",
    "    for column in HEADER:\n",
    "        if column not in [WEIGHT_COLUMN_NAME, TARGET_NAME]:\n",
    "            if column in NUMERIC_FEATURE_NAMES:\n",
    "                numeric_input = tf.keras.layers.Input(shape=(1, ), name=column, dtype='float32')\n",
    "                inputs.append(numeric_input)\n",
    "                to_concat.append(numeric_input)\n",
    "            else:\n",
    "                categorical_input = tf.keras.layers.Input(shape=(1, ), name=column, dtype='int32')\n",
    "                inputs.append(categorical_input)\n",
    "                \n",
    "#                 categorical_input_index = tf.keras.layers.Lambda(\n",
    "#                     lambda string: mapping_tables[column].lookup(string))(categorical_input)\n",
    "\n",
    "                vocabulary_size = len(feature_vocabolary[column])\n",
    "                embed_size = int(math.sqrt(vocabulary_size))\n",
    "                embedding = tf.keras.layers.Embedding(input_dim=vocabulary_size, \n",
    "                                                      output_dim=embed_size)(categorical_input)\n",
    "                reshape = tf.keras.layers.Reshape(target_shape=(embed_size, ))(embedding)\n",
    "                to_concat.append(reshape)\n",
    "                    \n",
    "    input_layer = tf.keras.layers.Concatenate(-1)(to_concat)    \n",
    "    dense_inputs = input_layer\n",
    "    for i in range(len(params.hidden_units)):\n",
    "        dense = tf.keras.layers.Dense(params.hidden_units[i], activation='relu')(dense_inputs)\n",
    "        dense_dropout = tf.keras.layers.Dropout(params.dropout_prob)(dense)#, training=is_training)\n",
    "        dense_inputs = dense_dropout\n",
    "    fully_connected = dense_inputs  \n",
    "    logits = tf.keras.layers.Dense(units=1, name='logits', activation=None)(fully_connected)\n",
    "    \n",
    "    sigmoid = tf.keras.layers.Activation(activation='sigmoid', name='probability')(logits)\n",
    "\n",
    "    # keras model\n",
    "    model = tf.keras.models.Model(inputs=inputs, outputs=sigmoid)\n",
    "    \n",
    "    model.compile(\n",
    "        loss='binary_crossentropy', \n",
    "        optimizer='adam', \n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: models/census/dnn_classifier/checkpoints: File exists\n",
      "checkpoint                          model.ckpt-0.index\n",
      "\u001b[34mcheckpoints\u001b[m\u001b[m                         model.ckpt-0.meta\n",
      "\u001b[34mexport\u001b[m\u001b[m                              model.ckpt-1000.data-00000-of-00002\n",
      "graph.pbtxt                         model.ckpt-1000.data-00001-of-00002\n",
      "keras_classifier.h5                 model.ckpt-1000.index\n",
      "model.ckpt-0.data-00000-of-00002    model.ckpt-1000.meta\n",
      "model.ckpt-0.data-00001-of-00002\n"
     ]
    }
   ],
   "source": [
    "!mkdir $model_dir/checkpoints\n",
    "!ls $model_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Define callbacks \n",
    "1. Early stopping callback\n",
    "2. Checkpoints callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [\n",
    "    tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=2),\n",
    "    tf.keras.callbacks.ModelCheckpoint(\n",
    "        os.path.join(model_dir,'checkpoints', 'model-{epoch:02d}.h5'), \n",
    "        monitor='val_loss', \n",
    "        period=1)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "workclass (InputLayer)          (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "education (InputLayer)          (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "marital_status (InputLayer)     (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "occupation (InputLayer)         (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "relationship (InputLayer)       (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "race (InputLayer)               (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "gender (InputLayer)             (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "native_country (InputLayer)     (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_16 (Embedding)        (None, 1, 3)         27          workclass[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "embedding_17 (Embedding)        (None, 1, 4)         64          education[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "embedding_18 (Embedding)        (None, 1, 2)         14          marital_status[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "embedding_19 (Embedding)        (None, 1, 3)         45          occupation[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "embedding_20 (Embedding)        (None, 1, 2)         12          relationship[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "embedding_21 (Embedding)        (None, 1, 2)         10          race[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "embedding_22 (Embedding)        (None, 1, 1)         2           gender[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "embedding_23 (Embedding)        (None, 1, 6)         252         native_country[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "age (InputLayer)                (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "reshape_16 (Reshape)            (None, 3)            0           embedding_16[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "reshape_17 (Reshape)            (None, 4)            0           embedding_17[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "education_num (InputLayer)      (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "reshape_18 (Reshape)            (None, 2)            0           embedding_18[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "reshape_19 (Reshape)            (None, 3)            0           embedding_19[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "reshape_20 (Reshape)            (None, 2)            0           embedding_20[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "reshape_21 (Reshape)            (None, 2)            0           embedding_21[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "reshape_22 (Reshape)            (None, 1)            0           embedding_22[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "capital_gain (InputLayer)       (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "capital_loss (InputLayer)       (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "hours_per_week (InputLayer)     (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "reshape_23 (Reshape)            (None, 6)            0           embedding_23[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 28)           0           age[0][0]                        \n",
      "                                                                 reshape_16[0][0]                 \n",
      "                                                                 reshape_17[0][0]                 \n",
      "                                                                 education_num[0][0]              \n",
      "                                                                 reshape_18[0][0]                 \n",
      "                                                                 reshape_19[0][0]                 \n",
      "                                                                 reshape_20[0][0]                 \n",
      "                                                                 reshape_21[0][0]                 \n",
      "                                                                 reshape_22[0][0]                 \n",
      "                                                                 capital_gain[0][0]               \n",
      "                                                                 capital_loss[0][0]               \n",
      "                                                                 hours_per_week[0][0]             \n",
      "                                                                 reshape_23[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 100)          2900        concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_6 (Dropout)             (None, 100)          0           dense_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_7 (Dense)                 (None, 70)           7070        dropout_6[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_7 (Dropout)             (None, 70)           0           dense_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_8 (Dense)                 (None, 50)           3550        dropout_7[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_8 (Dropout)             (None, 50)           0           dense_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "logits (Dense)                  (None, 1)            51          dropout_8[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "probability (Activation)        (None, 1)            0           logits[0][0]                     \n",
      "==================================================================================================\n",
      "Total params: 13,997\n",
      "Trainable params: 13,997\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.utils.training_utils import multi_gpu_model\n",
    "model = create_model(params)\n",
    "# model = multi_gpu_model(model, gpus=4) # This is to train the model with multiple GPUs\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Run experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "163/163 [==============================] - 33s 201ms/step - loss: 1.5543 - acc: 0.7222 - val_loss: 1.3147 - val_acc: 0.7827\n",
      "Epoch 2/5\n",
      "163/163 [==============================] - 2s 15ms/step - loss: 1.3254 - acc: 0.7739 - val_loss: 1.1907 - val_acc: 0.7828\n",
      "Epoch 3/5\n",
      "163/163 [==============================] - 2s 14ms/step - loss: 1.2045 - acc: 0.7826 - val_loss: 1.1444 - val_acc: 0.8144\n",
      "Epoch 4/5\n",
      "163/163 [==============================] - 2s 14ms/step - loss: 1.1789 - acc: 0.7995 - val_loss: 1.1281 - val_acc: 0.8184\n",
      "Epoch 5/5\n",
      "163/163 [==============================] - 2s 12ms/step - loss: 1.3902 - acc: 0.7898 - val_loss: 1.4400 - val_acc: 0.7989\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x14cdced10>"
      ]
     },
     "execution_count": 316,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = make_keras_input_fn(\n",
    "    TRAIN_DATA_FILE,\n",
    "    batch_size=params.batch_size,\n",
    "    mode=tf.estimator.ModeKeys.TRAIN\n",
    ")()\n",
    "\n",
    "\n",
    "valid_data = make_keras_input_fn(\n",
    "    EVAL_DATA_FILE,\n",
    "    batch_size=params.batch_size,\n",
    "    mode=tf.estimator.ModeKeys.EVAL\n",
    ")()\n",
    "\n",
    "steps_per_epoch = int(math.ceil(TRAIN_DATA_SIZE/float(params.batch_size)))\n",
    "model.fit(\n",
    "    train_data, \n",
    "    epochs=5, \n",
    "    steps_per_epoch=steps_per_epoch,\n",
    "    validation_data=valid_data,\n",
    "    validation_steps=steps_per_epoch,\n",
    "    callbacks=callbacks\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.01-1.31.hdf5 model.03-1.14.hdf5 model.05-1.44.hdf5\r\n",
      "model.02-1.19.hdf5 model.04-1.13.hdf5\r\n"
     ]
    }
   ],
   "source": [
    "!ls $model_dir/checkpoints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save and Load Keras Model for Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keras model saved to: models/census/dnn_classifier/keras_classifier.h5\n",
      "Keras model loaded.\n"
     ]
    }
   ],
   "source": [
    "keras_model_dir = os.path.join(model_dir, 'keras_classifier.h5')\n",
    "model.save(keras_model_dir)\n",
    "print(\"Keras model saved to: {}\".format(keras_model_dir))\n",
    "model = tf.keras.models.load_model(keras_model_dir)\n",
    "print(\"Keras model loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' <=50K', ' <=50K', ' >50K', ' <=50K', ' <=50K']\n"
     ]
    }
   ],
   "source": [
    "predict_data = make_keras_input_fn(\n",
    "        EVAL_DATA_FILE,\n",
    "        batch_size=5,\n",
    "        mode=tf.estimator.ModeKeys.EVAL\n",
    "    )()\n",
    "\n",
    "predictions = map(\n",
    "    lambda probability: TARGET_LABELS[0] if probability <0.5 else TARGET_LABELS[1], \n",
    "    model.predict(predict_data, steps=1)\n",
    ")\n",
    "\n",
    "print(list(predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export Keras Model as saved_model for tf.Serving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:This model was compiled with a Keras optimizer (<tensorflow.python.keras.optimizers.Adam object at 0x154e845d0>) but is being saved in TensorFlow format with `save_weights`. The model's weights will be saved, but unlike with TensorFlow optimizers in the TensorFlow format the optimizer's state will not be saved.\n",
      "\n",
      "Consider using a TensorFlow optimizer from `tf.train`.\n",
      "WARNING:tensorflow:Model was compiled with an optimizer, but the optimizer is not from `tf.train` (e.g. `tf.train.AdagradOptimizer`). Only the serving graph was exported. The train and evaluate graphs were not added to the SavedModel.\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Eval: None\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Classify: None\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Regress: None\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Predict: ['serving_default']\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Train: None\n",
      "INFO:tensorflow:No assets to save.\n",
      "INFO:tensorflow:No assets to write.\n",
      "INFO:tensorflow:SavedModel written to: models/census/dnn_classifier/export/temp-1547755904/saved_model.pb\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'models/census/dnn_classifier/export/1547755904'"
      ]
     },
     "execution_count": 292,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.environ['MODEL_DIR'] = model_dir\n",
    "export_dir = os.path.join(model_dir, 'export')\n",
    "\n",
    "from tensorflow.contrib.saved_model.python.saved_model import keras_saved_model\n",
    "keras_saved_model.save_keras_model(model, export_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "models/census/dnn_classifier/export/1547745765\n",
      "assets\n",
      "saved_model.pb\n",
      "variables\n",
      "\n",
      "MetaGraphDef with tag-set: 'serve' contains the following SignatureDefs:\n",
      "\n",
      "signature_def['serving_default']:\n",
      "  The given SavedModel SignatureDef contains the following input(s):\n",
      "    inputs['age'] tensor_info:\n",
      "        dtype: DT_FLOAT\n",
      "        shape: (-1, 1)\n",
      "        name: age:0\n",
      "    inputs['capital_gain'] tensor_info:\n",
      "        dtype: DT_FLOAT\n",
      "        shape: (-1, 1)\n",
      "        name: capital_gain:0\n",
      "    inputs['capital_loss'] tensor_info:\n",
      "        dtype: DT_FLOAT\n",
      "        shape: (-1, 1)\n",
      "        name: capital_loss:0\n",
      "    inputs['education'] tensor_info:\n",
      "        dtype: DT_FLOAT\n",
      "        shape: (-1, 1)\n",
      "        name: education:0\n",
      "    inputs['education_num'] tensor_info:\n",
      "        dtype: DT_FLOAT\n",
      "        shape: (-1, 1)\n",
      "        name: education_num:0\n",
      "    inputs['gender'] tensor_info:\n",
      "        dtype: DT_FLOAT\n",
      "        shape: (-1, 1)\n",
      "        name: gender:0\n",
      "    inputs['hours_per_week'] tensor_info:\n",
      "        dtype: DT_FLOAT\n",
      "        shape: (-1, 1)\n",
      "        name: hours_per_week:0\n",
      "    inputs['marital_status'] tensor_info:\n",
      "        dtype: DT_FLOAT\n",
      "        shape: (-1, 1)\n",
      "        name: marital_status:0\n",
      "    inputs['native_country'] tensor_info:\n",
      "        dtype: DT_FLOAT\n",
      "        shape: (-1, 1)\n",
      "        name: native_country:0\n",
      "    inputs['occupation'] tensor_info:\n",
      "        dtype: DT_FLOAT\n",
      "        shape: (-1, 1)\n",
      "        name: occupation:0\n",
      "    inputs['race'] tensor_info:\n",
      "        dtype: DT_FLOAT\n",
      "        shape: (-1, 1)\n",
      "        name: race:0\n",
      "    inputs['relationship'] tensor_info:\n",
      "        dtype: DT_FLOAT\n",
      "        shape: (-1, 1)\n",
      "        name: relationship:0\n",
      "    inputs['workclass'] tensor_info:\n",
      "        dtype: DT_FLOAT\n",
      "        shape: (-1, 1)\n",
      "        name: workclass:0\n",
      "  The given SavedModel SignatureDef contains the following output(s):\n",
      "    outputs['probability'] tensor_info:\n",
      "        dtype: DT_FLOAT\n",
      "        shape: (-1, 1)\n",
      "        name: probability/Sigmoid:0\n",
      "  Method name is: tensorflow/serving/predict\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "saved_models_base=${MODEL_DIR}/export/\n",
    "saved_model_dir=${saved_models_base}$(ls ${saved_models_base} | tail -n 1)\n",
    "echo ${saved_model_dir}\n",
    "ls ${saved_model_dir}\n",
    "saved_model_cli show --dir=${saved_model_dir} --all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert to Estimator for Distributed Training..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using default config.\n",
      "WARNING:tensorflow:Using temporary folder as model directory: /var/folders/hp/gzm_7hs931v5kt53p6rywh5w00fqrl/T/tmpXJ65ph\n",
      "INFO:tensorflow:Using the Keras model provided.\n",
      "INFO:tensorflow:Using config: {'_save_checkpoints_secs': 600, '_num_ps_replicas': 0, '_keep_checkpoint_max': 5, '_task_type': 'worker', '_global_id_in_cluster': 0, '_is_chief': True, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x13ad74a90>, '_model_dir': '/var/folders/hp/gzm_7hs931v5kt53p6rywh5w00fqrl/T/tmpXJ65ph', '_protocol': None, '_save_checkpoints_steps': None, '_keep_checkpoint_every_n_hours': 10000, '_service': None, '_session_config': allow_soft_placement: true\n",
      "graph_options {\n",
      "  rewrite_options {\n",
      "    meta_optimizer_iterations: ONE\n",
      "  }\n",
      "}\n",
      ", '_tf_random_seed': None, '_save_summary_steps': 100, '_device_fn': None, '_experimental_distribute': None, '_num_worker_replicas': 1, '_task_id': 0, '_log_step_count_steps': 100, '_evaluation_master': '', '_eval_distribute': None, '_train_distribute': None, '_master': ''}\n"
     ]
    }
   ],
   "source": [
    "estimator = tf.keras.estimator.model_to_estimator(model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
